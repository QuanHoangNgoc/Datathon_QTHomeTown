{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7803789,"sourceType":"datasetVersion","datasetId":4569729},{"sourceType":"datasetVersion","sourceId":7824744,"datasetId":4585011}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"accelerator":"TPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"test\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vymEjuKIk4pd","outputId":"f43c8496-3b88-4698-aafc-f640278d0eea","execution":{"iopub.status.busy":"2024-03-12T13:36:12.823543Z","iopub.execute_input":"2024-03-12T13:36:12.823879Z","iopub.status.idle":"2024-03-12T13:36:12.837328Z","shell.execute_reply.started":"2024-03-12T13:36:12.823850Z","shell.execute_reply":"2024-03-12T13:36:12.836342Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"test\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### global","metadata":{}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport torch\nprint(tf.__version__)\nprint(torch.__version__)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjC16i_mk4pi","outputId":"d2f057df-b6d6-4534-a7f5-80cad2abc6c1","execution":{"iopub.status.busy":"2024-03-12T13:36:12.842876Z","iopub.execute_input":"2024-03-12T13:36:12.843305Z","iopub.status.idle":"2024-03-12T13:36:18.603929Z","shell.execute_reply.started":"2024-03-12T13:36:12.843269Z","shell.execute_reply":"2024-03-12T13:36:18.602998Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-12 13:36:13.179964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-12 13:36:13.180026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-12 13:36:13.181554: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"2.15.0\n2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nprint(np.__version__)\n# setting random_state\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntf.random.set_seed(RANDOM_STATE)\ntorch.manual_seed(RANDOM_STATE)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yf2_vC6Kk4pj","outputId":"0150f0ee-2934-4080-a25a-383aa7018bc8","execution":{"iopub.status.busy":"2024-03-12T13:36:18.605108Z","iopub.execute_input":"2024-03-12T13:36:18.605619Z","iopub.status.idle":"2024-03-12T13:36:18.615409Z","shell.execute_reply.started":"2024-03-12T13:36:18.605593Z","shell.execute_reply":"2024-03-12T13:36:18.614483Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1.26.4\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7dddcceadf90>"},"metadata":{}}]},{"cell_type":"markdown","source":"### some libraries and functions","metadata":{"id":"db9CUa5Ek4pl"}},{"cell_type":"code","source":"# libraries\nimport sys, math\nfrom collections import defaultdict\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn","metadata":{"id":"r7nwh0e_k4pm","execution":{"iopub.status.busy":"2024-03-12T13:36:18.618212Z","iopub.execute_input":"2024-03-12T13:36:18.618475Z","iopub.status.idle":"2024-03-12T13:36:18.963664Z","shell.execute_reply.started":"2024-03-12T13:36:18.618452Z","shell.execute_reply":"2024-03-12T13:36:18.962778Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# fix random_state\ndef fixRandomState(fixed_state: int=RANDOM_STATE):\n  np.random.seed(fixed_state)\n  tf.random.set_seed(fixed_state)\n  torch.manual_seed(fixed_state)\n\n# exception\ndef exception(requirement: bool, content):\n  if(requirement == False): raise ValueError(content)\ndef catchException(ex: Exception):\n  print(type(ex), ex.args)\n  exception(False, ex)\n\n# message\ndef mesVerbose(flag: bool, verbose, func_dir: str=\"\"):\n  if(flag == False): return\n  print(\"__verbose__:\", func_dir, verbose)\ndef mesWarningToUser(note, func_dir: str=\"\"):\n  print(\"__warning__:\", func_dir, str(note) + \"&&&\")","metadata":{"id":"om06RJL2k4pm","execution":{"iopub.status.busy":"2024-03-12T13:36:18.964815Z","iopub.execute_input":"2024-03-12T13:36:18.965384Z","iopub.status.idle":"2024-03-12T13:36:18.974529Z","shell.execute_reply.started":"2024-03-12T13:36:18.965352Z","shell.execute_reply":"2024-03-12T13:36:18.973468Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def over(val, name=\"\") -> tuple:\n  try: mesVerbose(True, (type(val), val.shape, str(sys.getsizeof(val)) + \"Bytes\"), name)\n  except: mesVerbose(True, (type(val), \"no-shape\", str(sys.getsizeof(val)) + \"Bytes\"), name)","metadata":{"id":"1lElqLSLk4pn","execution":{"iopub.status.busy":"2024-03-12T13:36:18.975636Z","iopub.execute_input":"2024-03-12T13:36:18.975919Z","iopub.status.idle":"2024-03-12T13:36:18.985938Z","shell.execute_reply.started":"2024-03-12T13:36:18.975893Z","shell.execute_reply":"2024-03-12T13:36:18.985142Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### model architecture","metadata":{"id":"ZSU76Uxgk4po"}},{"cell_type":"code","source":"from torch import nn, optim\nfrom torch.utils import data\n\nBATCH_SIZE = 16\nIN_SHAPE = (BATCH_SIZE, 3, 224, 224)\n\nYOLO_BACKBONE_ARCHITECTURE = [(64, 7, 2, 'same'), 'M',\n                                (192, 3, 1, 'same'), 'M',\n                                (128, 1, 1, 'valid'),\n                                [(128, 256), 1],\n                                [(256, 512), 1], 'M',\n                                [(256, 512), 4],\n                                [(512, 1024), 1], 'M',\n                                [(512, 1024), 2]]\n\nGRID_SIZE = 7\nNUM_BOXES = 2\nNUM_CLASSES = 3\nOUT_SHAPE = (BATCH_SIZE, 7, 7, 8)","metadata":{"id":"LXmFP8FWk4pp","execution":{"iopub.status.busy":"2024-03-12T13:36:18.987045Z","iopub.execute_input":"2024-03-12T13:36:18.987375Z","iopub.status.idle":"2024-03-12T13:36:18.999662Z","shell.execute_reply.started":"2024-03-12T13:36:18.987337Z","shell.execute_reply":"2024-03-12T13:36:18.998866Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SZN5eJek4pv","outputId":"a12ecf46-47e8-40e1-8ab4-3c4c16fd3998","execution":{"iopub.status.busy":"2024-03-12T13:36:19.000644Z","iopub.execute_input":"2024-03-12T13:36:19.000967Z","iopub.status.idle":"2024-03-12T13:36:19.066938Z","shell.execute_reply.started":"2024-03-12T13:36:19.000936Z","shell.execute_reply":"2024-03-12T13:36:19.065889Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"##### nnModule","metadata":{"id":"5bdgFDackvjY"}},{"cell_type":"code","source":"class nnModule(nn.Module):\n  def unittest_backward(self):\n    mesVerbose(True, \"@@@ test backward\", \"nnModule > unittest_backward:\")\n    in_shape = self.getInShape()\n    model = self.getModel()\n    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3]).to(DEVICE)\n    out = self.forward(x)\n    loss = nn.MSELoss()(out, torch.rand(*out.shape).to(DEVICE))\n\n    print(\"example_mse_loss:\", type(loss), loss)\n    optimizer = optim.Adam(list(model.parameters()), lr=2e-5, weight_decay=0)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(\"backward finish, the weights (state) of current instance CAN changed!\")\n    params = list(model.parameters())\n    print(\"parameters():\", str(len(params)), str(sys.getsizeof(params))) \n\n  def unittest_summary(self):\n    mesVerbose(True, \"@@@ test summary\", \"nnModule > unittest_summary:\")\n    in_shape = self.getInShape()\n    model = self.getModel()\n    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3]).to(DEVICE)\n    for layer in model:\n      print(\"\\tin_shape:\", type(x), x.shape)\n      print(type(layer), sys.getsizeof(layer))\n      x = layer(x)\n    print(\"out_shape:\", type(x), x.shape)\n    \n  def __init__(self) -> None:\n    super(nnModule, self).__init__()\n    self.in_shape = ()\n    self.out_shape = ()\n    self.model = nn.ModuleList()\n\n  def getInShape(self): return self.in_shape\n  def getOutShape(self): return self.out_shape\n  def getModel(self): return self.model\n  def setInShape(self, in_shape): self.in_shape = in_shape\n  def setOutShape(self, out_shape): self.out_shape = out_shape\n  def setModel(self, model): self.model = model\n    \n  def forward(self, x): \n    x = x.to(DEVICE) \n    for layer in self.getModel(): \n        layer = layer.to(DEVICE) \n        x = layer(x) \n    return x \n","metadata":{"id":"QXDFK-vck4pr","execution":{"iopub.status.busy":"2024-03-12T13:36:19.068311Z","iopub.execute_input":"2024-03-12T13:36:19.068686Z","iopub.status.idle":"2024-03-12T13:36:19.084161Z","shell.execute_reply.started":"2024-03-12T13:36:19.068652Z","shell.execute_reply":"2024-03-12T13:36:19.083285Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"##### blcoks","metadata":{"id":"y5MIjFYCk4pp"}},{"cell_type":"code","source":"class ConvWithBatchNorm(nnModule):\n  \"\"\"Conv layer with batch norm and leaky relu\"\"\"\n\n  def __init__(self, in_c: int, out_c: int, k_size: int, stride=1, negative_slope=0.1):\n    super(ConvWithBatchNorm, self).__init__()\n\n    padding = k_size // 2\n    layers = self.getModel()\n    layers += [nn.Conv2d(in_c, out_c, k_size, stride=stride, padding=padding, bias=False)]\n    layers += [nn.BatchNorm2d(num_features=out_c)]\n    layers += [nn.LeakyReLU(negative_slope=negative_slope)]\n    self.setModel(layers) ","metadata":{"id":"r5mESj1bk4pq","execution":{"iopub.status.busy":"2024-03-12T13:36:19.085320Z","iopub.execute_input":"2024-03-12T13:36:19.086114Z","iopub.status.idle":"2024-03-12T13:36:19.097921Z","shell.execute_reply.started":"2024-03-12T13:36:19.086086Z","shell.execute_reply":"2024-03-12T13:36:19.097110Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class BottleNeckBlock(nnModule):\n  \"\"\"Block of 1x1 reduction layers followed by 3x3 conv. layer\"\"\"\n\n  def __init__(self, in_c: int, out_ces: tuple, num_repeat: int):\n    super(BottleNeckBlock, self).__init__()\n\n    out_1x1 = out_ces[0]\n    out_3x3 = out_ces[1]\n    layers = self.getModel() \n    for i in range(num_repeat):\n      layers += [nn.Conv2d(in_c, out_1x1, 1, stride=1, padding=0, bias=False)]\n      layers += [nn.Conv2d(out_1x1, out_3x3, 3, stride=1, padding=1, bias=False)]\n    self.setModel(layers) ","metadata":{"id":"Nj0k_pkUk4pq","execution":{"iopub.status.busy":"2024-03-12T13:36:19.099068Z","iopub.execute_input":"2024-03-12T13:36:19.099357Z","iopub.status.idle":"2024-03-12T13:36:19.110785Z","shell.execute_reply.started":"2024-03-12T13:36:19.099333Z","shell.execute_reply":"2024-03-12T13:36:19.109997Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"##### YoloBackbone","metadata":{"id":"g7_sw1k3k4pr"}},{"cell_type":"code","source":"class YoloBackbone(nnModule):\n  \"\"\"YOLO backbone extract feature from the input\"\"\"\n\n  def __init__(self, in_shpae: tuple, backbone_config=YOLO_BACKBONE_ARCHITECTURE):\n    super(YoloBackbone, self).__init__()\n    self.setInShape(in_shpae)\n    model = self.getModel()\n    x = torch.rand(in_shpae[0], in_shpae[1], in_shpae[2], in_shpae[3]).to(DEVICE)\n    for i, config in enumerate(backbone_config):\n      if type(config) == tuple:\n        out_c, k_size, stride, _ = config\n        model += [ConvWithBatchNorm(in_c=x.shape[1], out_c=out_c, k_size=k_size, stride=stride, negative_slope=0.1)]\n        x = model[-1](x)\n\n      elif type(config) == str:\n        model += [nn.MaxPool2d(kernel_size=2, stride=2, padding=0)]\n        x = model[-1](x)\n\n      elif type(config) == list:\n        out_ces, num_repeat = config\n        model += [BottleNeckBlock(x.shape[1], out_ces, num_repeat)]\n        x = model[-1](x)\n    self.setOutShape(x.shape)\n    self.setModel(model=model)","metadata":{"id":"0NT7ITSLk4pr","execution":{"iopub.status.busy":"2024-03-12T13:36:19.112086Z","iopub.execute_input":"2024-03-12T13:36:19.112418Z","iopub.status.idle":"2024-03-12T13:36:19.121842Z","shell.execute_reply.started":"2024-03-12T13:36:19.112380Z","shell.execute_reply":"2024-03-12T13:36:19.121083Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"YoloBackbone((4, 3, 224, 224)).unittest_backward()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7uJuu6Ak4ps","outputId":"3a42565d-4051-4e38-fc4b-980ca5074d3e","execution":{"iopub.status.busy":"2024-03-12T13:36:19.126062Z","iopub.execute_input":"2024-03-12T13:36:19.126352Z","iopub.status.idle":"2024-03-12T13:36:20.895885Z","shell.execute_reply.started":"2024-03-12T13:36:19.126326Z","shell.execute_reply":"2024-03-12T13:36:20.894911Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"__verbose__: nnModule > unittest_backward: @@@ test backward\nexample_mse_loss: <class 'torch.Tensor'> tensor(0.3340, device='cuda:0', grad_fn=<MseLossBackward0>)\nbackward finish, the weights (state) of current instance CAN changed!\nparameters(): 27 312\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### YoloOutput","metadata":{"id":"cyoC7afXk4ps"}},{"cell_type":"code","source":"YOLO_OUT_ARCHITECTURE = [(4096, 0.1), 0.5, (2040, 0.1), 0.5, (1024, 0.1), 0.5, (GRID_SIZE * GRID_SIZE * (NUM_BOXES * 5 + NUM_CLASSES), 0.1)]","metadata":{"id":"qXT-6EExk4ps","execution":{"iopub.status.busy":"2024-03-12T13:36:20.897381Z","iopub.execute_input":"2024-03-12T13:36:20.897781Z","iopub.status.idle":"2024-03-12T13:36:20.903930Z","shell.execute_reply.started":"2024-03-12T13:36:20.897745Z","shell.execute_reply":"2024-03-12T13:36:20.902931Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class YoloOutput(nnModule):\n  \"\"\"YOLO last convolution and FC layers to produce prediction\"\"\"\n\n  def __init__(self, in_shape: tuple):\n    super(YoloOutput, self).__init__()\n    self.setInShape(in_shape=in_shape)\n    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3]).to(DEVICE)\n    model = self.getModel() \n    model += [ConvWithBatchNorm(in_shape[1], out_c=1024, k_size=3),\n              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n              nn.Flatten()]\n    self.setModel(model) \n    x = self.forward(x) \n\n    for i, config in enumerate(YOLO_OUT_ARCHITECTURE):\n      if type(config) == tuple:\n        out_f, slop = config\n        model += [nn.Linear(in_features=x.shape[1], out_features=out_f), nn.LeakyReLU(negative_slope=slop)]\n        x = model[-1].to(DEVICE)(model[-2].to(DEVICE)(x))\n      else:\n        p = config\n        model += [nn.Dropout(p=0.5)]\n        x = model[-1](x)\n\n    self.setOutShape(x.shape)\n    self.setModel(model)","metadata":{"id":"rqf_8jvrk4pt","execution":{"iopub.status.busy":"2024-03-12T13:36:20.905252Z","iopub.execute_input":"2024-03-12T13:36:20.905538Z","iopub.status.idle":"2024-03-12T13:36:20.917303Z","shell.execute_reply.started":"2024-03-12T13:36:20.905512Z","shell.execute_reply":"2024-03-12T13:36:20.916217Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"YoloOutput((16, 1024, 7, 7)).unittest_backward()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtBJgOFhk4pt","outputId":"9ba49b90-093b-41ff-90ec-93431a959b74","execution":{"iopub.status.busy":"2024-03-12T13:36:20.918356Z","iopub.execute_input":"2024-03-12T13:36:20.918638Z","iopub.status.idle":"2024-03-12T13:36:23.768088Z","shell.execute_reply.started":"2024-03-12T13:36:20.918612Z","shell.execute_reply":"2024-03-12T13:36:23.767018Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"__verbose__: nnModule > unittest_backward: @@@ test backward\nexample_mse_loss: <class 'torch.Tensor'> tensor(0.3095, device='cuda:0', grad_fn=<MseLossBackward0>)\nbackward finish, the weights (state) of current instance CAN changed!\nparameters(): 20 248\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### YoloV1","metadata":{"id":"cthP7XbNk4pt"}},{"cell_type":"code","source":"class YoloV1(nnModule):\n  \"\"\"End-to-end YOLO network\"\"\"\n\n  def __init__(self, in_shape: tuple):\n    super(YoloV1, self).__init__()\n    self.setInShape(in_shape)\n\n    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3]).to(DEVICE)\n    yolo_backbone = YoloBackbone(in_shape)\n    x = yolo_backbone(x)\n    yolo_output = YoloOutput(in_shape=x.shape)\n    x = yolo_output(x)\n\n    self.setOutShape(x.shape)\n    model = self.getModel()\n    model += [yolo_backbone, yolo_output]\n    self.setModel(model)","metadata":{"id":"ynLF5B34k4pt","execution":{"iopub.status.busy":"2024-03-12T13:36:23.769216Z","iopub.execute_input":"2024-03-12T13:36:23.769512Z","iopub.status.idle":"2024-03-12T13:36:23.776330Z","shell.execute_reply.started":"2024-03-12T13:36:23.769486Z","shell.execute_reply":"2024-03-12T13:36:23.775369Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"YoloV1((16, 3, 224, 224)).to(DEVICE).unittest_backward()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiyS607xk4pu","outputId":"2a457892-93b1-4ba2-c0d0-fc1ec331defe","execution":{"iopub.status.busy":"2024-03-12T13:36:23.777568Z","iopub.execute_input":"2024-03-12T13:36:23.778153Z","iopub.status.idle":"2024-03-12T13:36:27.113083Z","shell.execute_reply.started":"2024-03-12T13:36:23.778119Z","shell.execute_reply":"2024-03-12T13:36:27.112147Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"__verbose__: nnModule > unittest_backward: @@@ test backward\nexample_mse_loss: <class 'torch.Tensor'> tensor(0.3073, device='cuda:0', grad_fn=<MseLossBackward0>)\nbackward finish, the weights (state) of current instance CAN changed!\nparameters(): 47 472\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### YoloLoss","metadata":{"id":"G4pfk6uRk4pu"}},{"cell_type":"code","source":"def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n  \"\"\"\n  Calculates intersection over union\n\n  Parameters:\n      boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n      boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n      box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.\n\n  Returns:\n      tensor: Intersection over union for all examples\n  \"\"\"\n  # boxes_preds shape is (N, 4)\n  # boxes_labels shape is (N, 4)\n\n  if box_format == 'midpoint':\n      box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n      box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n      box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n      box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n\n      box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n      box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n      box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n      box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n\n  if box_format == 'corners':\n      box1_x1 = boxes_preds[..., 0:1]\n      box1_y1 = boxes_preds[..., 1:2]\n      box1_x2 = boxes_preds[..., 2:3]\n      box1_y2 = boxes_preds[..., 3:4]\n\n      box2_x1 = boxes_labels[..., 0:1]\n      box2_y1 = boxes_labels[..., 1:2]\n      box2_x2 = boxes_labels[..., 2:3]\n      box2_y2 = boxes_labels[..., 3:4]\n\n  x1 = torch.max(box1_x1.to(DEVICE), box2_x1.to(DEVICE)).to(DEVICE)\n  y1 = torch.max(box1_y1.to(DEVICE), box2_y1.to(DEVICE)).to(DEVICE)\n  x2 = torch.min(box1_x2.to(DEVICE), box2_x2.to(DEVICE)).to(DEVICE)\n  y2 = torch.min(box1_y2.to(DEVICE), box2_y2.to(DEVICE)).to(DEVICE)\n\n  #&&& .clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n  intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n  box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1)).to(DEVICE)\n  box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1)).to(DEVICE)\n  return intersection / (box1_area + box2_area - intersection + 1e-6)\n","metadata":{"id":"aZpFmnwzNzRw","execution":{"iopub.status.busy":"2024-03-12T13:36:27.114321Z","iopub.execute_input":"2024-03-12T13:36:27.114605Z","iopub.status.idle":"2024-03-12T13:36:27.127304Z","shell.execute_reply.started":"2024-03-12T13:36:27.114580Z","shell.execute_reply":"2024-03-12T13:36:27.126375Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"EPS = 1e-6\ndef sign_sqrt(pred):\n  return torch.sign(pred) * torch.sqrt(torch.abs(pred + EPS))\n#&&& tai 0 khong co dao ham cua abs","metadata":{"id":"vDYzFCrMpqu7","execution":{"iopub.status.busy":"2024-03-12T13:36:27.128359Z","iopub.execute_input":"2024-03-12T13:36:27.128636Z","iopub.status.idle":"2024-03-12T13:36:27.143156Z","shell.execute_reply.started":"2024-03-12T13:36:27.128613Z","shell.execute_reply":"2024-03-12T13:36:27.142428Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n  def __init__(self, coord_c=5, noobj_c=0.5):\n    super(YoloLoss, self).__init__()\n    self.COORD = coord_c\n    self.NOOBJ = noobj_c\n    self.mse = nn.MSELoss(reduction=\"sum\")\n\n  def setLoss(self, some_loss): self.some_loss = some_loss\n  def getLoss(self): return self.some_loss\n\n  def forward(self, predictions: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    predictions, target = predictions.to(DEVICE), target.to(DEVICE)\n    predictions = predictions.reshape((-1, GRID_SIZE, GRID_SIZE, NUM_BOXES * 5 + NUM_CLASSES))\n    exists_box = target[..., [4]]\n    iou_b1 = intersection_over_union(\n        predictions[...,0:4], target[..., 0:4])\n    iou_b2 = intersection_over_union(\n        predictions[..., 5:9], target[..., 0:4])\n    bestbox = torch.where(iou_b1 >= iou_b2, 0, 1)\n\n    # class loss\n    class_loss = self.mse(\n      exists_box * predictions[..., 10:],\n      exists_box * target[..., 5:])\n\n    # obj loss\n    pred_box = (\n        (1-bestbox) * predictions[..., [4]] + (bestbox) * predictions[..., [9]]\n    )\n    object_loss = self.mse(\n      exists_box * pred_box,\n      exists_box * target[..., [4]]\n    )\n\n    # coor loss\n    pred_box = (\n        (1-bestbox) * predictions[..., 0:4] + (bestbox) * predictions[..., 5:9]\n    )\n    true_box = target[..., 0:4]\n    pred_box[..., 2:4] = sign_sqrt(pred_box[..., 2:4])\n    true_box[..., 2:4] = sign_sqrt(true_box[..., 2:4])\n    coor_loss = self.mse(\n      #exists_box * pred_box, end_dim=-2),\n      #exists_box * true_box, end_dim=-2),\n      exists_box * pred_box, exists_box * true_box\n    )\n\n    # no obj loss\n    no_obj_loss = self.mse(\n      (1 - exists_box) * predictions[..., [4]], (1 - exists_box) * target[..., [4]]\n    )\n    no_obj_loss += self.mse(\n      (1 - exists_box) * predictions[..., [9]], (1 - exists_box) * target[..., [4]]\n    )\n    self.setLoss((class_loss, coor_loss, object_loss, no_obj_loss))\n    return class_loss + object_loss + self.COORD * coor_loss + self.NOOBJ * no_obj_loss\n\n  def unittest_loss_backloss(self):\n    mesVerbose(True, \"@@@ test loss and backloss\", \"YoloLoss > unittest_loss_backloss:\")\n    model = YoloV1(in_shape=IN_SHAPE)\n    x = torch.rand(*IN_SHAPE)\n    out = model(x)\n    y = torch.rand(*OUT_SHAPE)\n    loss = self.forward(out, y)\n\n    print(\"example_loss:\", type(loss), loss)\n    optimizer = optim.Adam(list(model.parameters()), lr=2e-5, weight_decay=0)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(\"loss and backloss finish\")\n","metadata":{"id":"q8AApm83k4pu","execution":{"iopub.status.busy":"2024-03-12T13:36:27.144316Z","iopub.execute_input":"2024-03-12T13:36:27.144580Z","iopub.status.idle":"2024-03-12T13:36:27.160734Z","shell.execute_reply.started":"2024-03-12T13:36:27.144557Z","shell.execute_reply":"2024-03-12T13:36:27.159847Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"YoloLoss().unittest_loss_backloss()","metadata":{"id":"FZHO6m97QZkg","outputId":"d2762672-a62f-49cb-e4f5-d923786d277a","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-12T13:36:27.161742Z","iopub.execute_input":"2024-03-12T13:36:27.162014Z","iopub.status.idle":"2024-03-12T13:36:30.946916Z","shell.execute_reply.started":"2024-03-12T13:36:27.161986Z","shell.execute_reply":"2024-03-12T13:36:30.946017Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"__verbose__: YoloLoss > unittest_loss_backloss: @@@ test loss and backloss\nexample_loss: <class 'torch.Tensor'> tensor(2272.4500, device='cuda:0', grad_fn=<AddBackward0>)\nloss and backloss finish\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### DataLoad","metadata":{"id":"rUZaJA6Kk4pw"}},{"cell_type":"code","source":"import os\nfrom xml.etree import ElementTree\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom functools import partial\nfrom keras.preprocessing.image import load_img, img_to_array\n\nclass_names = ['apple', 'banana', 'orange']\n\nclass DataLoad(data.Dataset):\n  def utest_loaddata(self):\n    mesVerbose(True, \"@@@ test load data\", \"DataLoad(nn.Dataset) > utest_loaddata:\")\n    print(\"repeat:\", self.repeat)\n    print(\"is aug:\", self.aug)\n    over(self.imgs, \"imgs = \")\n    over(self.labels, \"labels = \")\n\n  def utest_getdata(self):\n    mesVerbose(True, \"@@@ test get data\", \"DataLoad(nn.Dataset) > utest_getdata:\")\n    x, y = self.__getitem__(0)\n    over(x, \"x = \")\n    over(y, \"y = \")\n\n  def __init__(self, file_dir, repeat, aug=False) -> None:\n    super().__init__()\n    self.repeat, self.aug = repeat, aug\n    dataframe = self.get_dataframe(file_dir=file_dir)\n    self.imgs, self.labels = self.load_dataset(dataframe, input_shape=(224, 224, 3), #!!!\n                                                grid_size=GRID_SIZE) # np.ndarray\n    # repeat\n    for i in range(repeat):\n      self.imgs = np.concatenate((self.imgs, self.imgs), axis=0)\n      self.labels = np.concatenate((self.labels, self.labels), axis=0)\n    # aug\n    if(aug == True):\n      for i, img in enumerate(self.imgs):\n        label = self.labels[i]\n        self.imgs[i], self.labels[i] = self._apply_augmentation(img, label, seed=RANDOM_STATE)\n\n  def __len__(self):\n    return len(self.imgs)\n\n  def __getitem__(self, idx):\n      x, y = self.imgs[idx], self.labels[idx] # np.ndarray\n      x, y = tf.convert_to_tensor(x), tf.convert_to_tensor(y) # tf.tensor\n      x = torch.tensor(x.numpy(), dtype=torch.float32)  # torch.tensor\n      y = torch.tensor(y.numpy(), dtype=torch.float32)\n      return x, y\n\n\n  def get_dataframe(self, file_dir):\n    \"\"\"\n    Get the train/val/test dataframe which contains image\n    file names and annotations files. If `phase = train',\n    return train and val set\n    :param file_dir: File directory to create dataframe\n    :return file_df: Train or test dataframe\n    \"\"\"\n\n    img_files = [os.path.join(file_dir, img_file) for img_file\n                 in sorted(os.listdir(file_dir)) if img_file[-4:] == '.jpg']\n    annot_files = [img_file[:-4] + '.xml' for img_file in img_files]\n\n    img_file_series = pd.Series(img_files, name='Image_file')\n    annot_file_series = pd.Series(annot_files, name='Annotation_file')\n    file_df = pd.DataFrame(pd.concat([img_file_series, annot_file_series], axis=1))\n    return file_df\n\n  def prepare_image(self, filename, input_shape):\n    \"\"\"\n    Resize image to expected dimension, and opt. apply some random transformation.\n    :param filename: File name\n    :param input_shape: Shape expected by the model (image will be resize accordingly)\n    :return : 3D image array, pixel values from [0., 1.]\n    \"\"\"\n\n    img = img_to_array(load_img(filename, target_size=input_shape)) / 255.\n    img = np.einsum('ijk->kij', img)\n    return img\n\n  def convert_to_xywh(self, bboxes):\n    \"\"\"\n    Convert list of (xmin, ymin, xmax, ymax) to\n    (x_center, y_center, box_width, box_height)\n    :param bboxes: List of bounding boxes, each has 4\n    values (xmin, ymin, xmax, ymax)\n    :return boxes: List of bounding boxes, each has 4\n    values (x_center, y_center, box_width, box_height)\n    \"\"\"\n\n    boxes = list()\n    for box in bboxes:\n        xmin, ymin, xmax, ymax = box\n\n        # Compute width and height of box\n        box_width = xmax - xmin\n        box_height = ymax - ymin\n\n        # Compute x, y center\n        x_center = int(xmin + (box_width / 2))\n        y_center = int(ymin + (box_height / 2))\n\n        boxes.append((x_center, y_center, box_width, box_height))\n\n    return boxes\n\n  def extract_annotation_file(self, filename):\n    \"\"\"\n    Extract bounding boxes from an annotation file\n    :param filename: Annotation file name\n    :return boxes: List of bounding boxes in image, each box has\n    4 values (x_center, y_center, box_width, box_height)\n    :return classes: List of classes in image\n    :return width: Width of image\n    :return height: Height of image\n    \"\"\"\n\n    # Load and parse the file\n    tree = ElementTree.parse(filename)\n    # Get the root of the document\n    root = tree.getroot()\n    boxes = list()\n    classes = list()\n\n    # Extract each bounding box\n    for box in root.findall('.//object'):\n        cls = class_names.index(box.find('name').text)\n        xmin = int(box.find('bndbox/xmin').text)\n        ymin = int(box.find('bndbox/ymin').text)\n        xmax = int(box.find('bndbox/xmax').text)\n        ymax = int(box.find('bndbox/ymax').text)\n        coors = (xmin, ymin, xmax, ymax)\n        boxes.append(coors)\n        classes.append(cls)\n\n    boxes = self.convert_to_xywh(boxes)\n\n    # Get width and height of an image\n    width = int(root.find('.//size/width').text)\n    height = int(root.find('.//size/height').text)\n\n    # Some annotation files have set width and height by 0,\n    # so we need to load image and get it width and height\n    if (width == 0) or (height == 0):\n        img = load_img(filename[:-4] + '.jpg')\n        width, height = img.width, img.height\n\n    return boxes, classes, width, height\n\n  def convert_bboxes_to_tensor(self, bboxes, classes, img_width, img_height, grid_size=7):\n    \"\"\"\n    Convert list of bounding boxes to tensor target\n    :param bboxes: List of bounding boxes in image, each box has\n    4 values (x_center, y_center, box_width, box_height)\n    :param classes: List of class in image\n    :param img_width: Image's width\n    :param img_height: Image's height\n    :param grid_size: Grid size\n    :return target: Target tensor (grid_size x grid_size x (5 + num_classes))\n    \"\"\"\n\n    num_classes = len(class_names)\n    target = np.zeros(shape=(grid_size, grid_size, 5 + num_classes), dtype=np.float32)\n\n    for idx, bbox in enumerate(bboxes):\n        x_center, y_center, width, height = bbox\n\n        # Compute size of each cell in grid\n        cell_w, cell_h = img_width / grid_size, img_height / grid_size\n\n        # Determine cell i, j of bounding box\n        i, j = int(y_center / cell_h), int(x_center / cell_w)\n\n        # Compute value of x_center and y_center in cell\n        x, y = (x_center / cell_w) - j, (y_center / cell_h) - i\n\n        # Normalize width and height of bounding box\n        w_norm, h_norm = width / img_width, height / img_height\n\n        # Add bounding box to tensor\n        # Set x, y, w, h\n        target[i, j, :4] += (x, y, w_norm, h_norm)\n        # Set obj score\n        target[i, j, 4] = 1.\n        # Set class dist.\n        target[i, j, 5 + classes[idx]] = 1.\n    return target\n\n  def load_dataset(self, dataframe, input_shape, grid_size=7):\n    \"\"\"\n    Load img and target tensor\n    :param dataframe: Dataframe contains img files and annotation files\n    :param input_shape: Shape expected by the model (image will be resize accordingly)\n    :param grid_size: Grid size\n    :return dataset: Iterable dataset\n    \"\"\"\n\n    imgs, targets = list(), list()\n\n    for _, row in tqdm(dataframe.iterrows()):\n        img = self.prepare_image(row.Image_file, input_shape)\n        target = self.extract_annotation_file(row.Annotation_file)\n        target = self.convert_bboxes_to_tensor(*target, grid_size)\n        imgs.append(img)\n        targets.append(target)\n\n    imgs = np.array(imgs)\n    targets = np.array(targets)\n    return imgs, targets\n    # dataset = tf.data.Dataset.from_tensor_slices((imgs, targets))\n    # return dataset\n\n  def _apply_augmentation(self, image, target, seed=None):\n    \"\"\"\n    Apply random brightness and saturation on image\n    :param image: Image to augment\n    :param target: Target tensor\n    :param seed: Seed for random operation\n    :return : Processed data\n    \"\"\"\n\n    # Random bright & saturation change\n    image = tf.image.random_brightness(image, max_delta=0.1, seed=seed)\n    image = tf.image.random_saturation(image, lower=0.5, upper=1.5, seed=seed)\n\n    # Keeping pixel values in check\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\n    return image, target\n\n  def load_dataset_from_df(self, dataframe, batch_size=32, num_repeat=None, shuffle=False,\n                         input_shape=(448, 448, 3), grid_size=7, augment=False,\n                         seed=None):\n    \"\"\"\n    Instantiate dataset\n    :param dataframe: Dataframe contains img files and annotation files\n    :param batch_size: Batch size\n    :param num_epochs: Number of epochs (to repeat the iteration - infinite if None)\n    :param shuffle: Flag to shuffle the dataset (if True)\n    :param input_shape: Shape of the processed image\n    :param grid_size: Grid size\n    :param augment: Flag to apply some random augmentations to the image\n    :param seed: Random seed for operation\n    :return : Iterable dataset\n    \"\"\"\n\n    apply_augmentation = partial(self._apply_augmentation, seed=seed)\n    dataset = self.load_dataset(dataframe, input_shape, grid_size)\n    ### !!!\n    dataset = dataset.repeat(num_repeat)\n    if shuffle:\n        dataset = dataset.shuffle(1000, seed)\n    if augment:\n        dataset = dataset.map(apply_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"id":"Ffxoi8Bgk4pw","execution":{"iopub.status.busy":"2024-03-12T13:36:30.948442Z","iopub.execute_input":"2024-03-12T13:36:30.948754Z","iopub.status.idle":"2024-03-12T13:36:30.985865Z","shell.execute_reply.started":"2024-03-12T13:36:30.948728Z","shell.execute_reply":"2024-03-12T13:36:30.984896Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"oNYux31ctz8C","colab":{"base_uri":"https://localhost:8080/"},"outputId":"08cb52b3-52e7-4c8b-ce4f-4b803052b52b","execution":{"iopub.status.busy":"2024-03-12T13:36:30.987148Z","iopub.execute_input":"2024-03-12T13:36:30.987452Z","iopub.status.idle":"2024-03-12T13:36:30.999499Z","shell.execute_reply.started":"2024-03-12T13:36:30.987425Z","shell.execute_reply":"2024-03-12T13:36:30.998619Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# train_dir = '/content/drive/MyDrive/Colab Notebooks/My_Laptop_Data/fruits_dataset/train'\ntrain_dir = '/kaggle/input/dataset1/fruits_dataset/train'\ndataload = DataLoad(train_dir, aug=False, repeat=4)\ntrain_df = dataload.get_dataframe(train_dir)","metadata":{"id":"g7aCKNwWk4py","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae5329ae-bc0b-4693-9c4e-3c52a8c90122","execution":{"iopub.status.busy":"2024-03-12T13:36:31.000625Z","iopub.execute_input":"2024-03-12T13:36:31.000878Z","iopub.status.idle":"2024-03-12T13:36:35.350382Z","shell.execute_reply.started":"2024-03-12T13:36:31.000856Z","shell.execute_reply":"2024-03-12T13:36:35.349327Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"13it [00:00, 128.44it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n240it [00:02, 88.29it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"dataload.utest_loaddata()\ndataload.utest_getdata()","metadata":{"id":"bPpZvNzCS5Ct","outputId":"bd75707c-b6e4-416d-c39f-3dc7b3fc5c45","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-12T13:36:35.351731Z","iopub.execute_input":"2024-03-12T13:36:35.352091Z","iopub.status.idle":"2024-03-12T13:36:35.673167Z","shell.execute_reply.started":"2024-03-12T13:36:35.352061Z","shell.execute_reply":"2024-03-12T13:36:35.672025Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"__verbose__: DataLoad(nn.Dataset) > utest_loaddata: @@@ test load data\nrepeat: 4\nis aug: False\n__verbose__: imgs =  (<class 'numpy.ndarray'>, (3840, 3, 224, 224), '2312110240Bytes')\n__verbose__: labels =  (<class 'numpy.ndarray'>, (3840, 7, 7, 8), '6021280Bytes')\n__verbose__: DataLoad(nn.Dataset) > utest_getdata: @@@ test get data\n__verbose__: x =  (<class 'torch.Tensor'>, torch.Size([3, 224, 224]), '80Bytes')\n__verbose__: y =  (<class 'torch.Tensor'>, torch.Size([7, 7, 8]), '80Bytes')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming train_dataset is your training dataset\n# train_loader = DataLoader(dataset=dataload, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True, prefetch_factor=2)\n","metadata":{"id":"jNUQ__YOk4p0","execution":{"iopub.status.busy":"2024-03-12T13:36:35.674378Z","iopub.execute_input":"2024-03-12T13:36:35.675013Z","iopub.status.idle":"2024-03-12T13:36:35.681325Z","shell.execute_reply.started":"2024-03-12T13:36:35.674966Z","shell.execute_reply":"2024-03-12T13:36:35.680376Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Assuming train_dataset is your training dataset\ntrain_loader = data.DataLoader(dataset=dataload, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)","metadata":{"id":"QX3qRl7Pk4p0","execution":{"iopub.status.busy":"2024-03-12T13:36:35.682799Z","iopub.execute_input":"2024-03-12T13:36:35.683391Z","iopub.status.idle":"2024-03-12T13:36:35.690209Z","shell.execute_reply.started":"2024-03-12T13:36:35.683357Z","shell.execute_reply":"2024-03-12T13:36:35.689378Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"over(train_loader, \"train_loader=\")","metadata":{"id":"Ds3HlEGU1OnC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe2fc7f7-ef1f-49f0-f8dd-f32539b4b982","execution":{"iopub.status.busy":"2024-03-12T13:36:35.691389Z","iopub.execute_input":"2024-03-12T13:36:35.691702Z","iopub.status.idle":"2024-03-12T13:36:35.701161Z","shell.execute_reply.started":"2024-03-12T13:36:35.691662Z","shell.execute_reply":"2024-03-12T13:36:35.700264Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"__verbose__: train_loader= (<class 'torch.utils.data.dataloader.DataLoader'>, 'no-shape', '48Bytes')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### training and testing","metadata":{"id":"V9fYLiHwk4qC"}},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, loss_fn):\n  loop = tqdm(train_loader, leave=True)\n  mesVerbose(True, \"@@@ --- training ---\", \"train_fn:\")\n  print(\"Data was loaded\")\n  sum_loss = 0 \n  \n  for batch_idx, (x, y) in enumerate(loop):\n#     mesVerbose(True, \"@@@ --- training loop ---\", \"train_fn > loop:\")\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    out = model(x)\n    loss = loss_fn(out, y)\n\n#     print(\"some_loss = \", loss_fn.getLoss())\n#     print(\"loss = \", loss) \n    sum_loss = loss + sum_loss\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n  print(\"State of model was CHANGE!\")\n  print(f\"Mean loss of epoch was {sum_loss / (len(loop))}\")\n  return sum_loss / len(loop) ","metadata":{"id":"oUiDcu5jk4qF","execution":{"iopub.status.busy":"2024-03-12T13:53:58.760820Z","iopub.execute_input":"2024-03-12T13:53:58.761548Z","iopub.status.idle":"2024-03-12T13:53:58.768769Z","shell.execute_reply.started":"2024-03-12T13:53:58.761512Z","shell.execute_reply":"2024-03-12T13:53:58.767819Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# train_loader = data.DataLoader(dataset=dataload, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\nmodel = YoloV1(in_shape=IN_SHAPE).to(DEVICE)\noptimizer = optim.Adam(list(model.parameters()), lr=2e-5, weight_decay=0)\nloss_fn = YoloLoss().to(DEVICE)","metadata":{"id":"zZTm0-S-k4qG","execution":{"iopub.status.busy":"2024-03-12T13:38:44.652176Z","iopub.execute_input":"2024-03-12T13:38:44.652536Z","iopub.status.idle":"2024-03-12T13:38:47.715565Z","shell.execute_reply.started":"2024-03-12T13:38:44.652507Z","shell.execute_reply":"2024-03-12T13:38:47.714742Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_fn(train_loader, model, optimizer, loss_fn)","metadata":{"id":"euzVKy3--qkM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fddba94f-8788-4d20-e153-635de3ae90ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_in_epochs = [] \nfor epoch in range(10): \n  loss_in_epochs.append(train_fn(train_loader, model, optimizer, loss_fn))","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:54:07.066748Z","iopub.execute_input":"2024-03-12T13:54:07.067132Z","iopub.status.idle":"2024-03-12T14:02:15.185268Z","shell.execute_reply.started":"2024-03-12T13:54:07.067102Z","shell.execute_reply":"2024-03-12T14:02:15.184224Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:49<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 39.61650085449219\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 36.75287628173828\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 34.120052337646484\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 31.654245376586914\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 29.69878578186035\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 27.934179306030273\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 26.33564567565918\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 26.19471549987793\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 24.75640296936035\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/240 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"__verbose__: train_fn: @@@ --- training ---\nData was loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:48<00:00,  4.94it/s]","output_type":"stream"},{"name":"stdout","text":"State of model was CHANGE!\nMean loss of epoch was 23.229841232299805\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model1.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-12T14:04:14.522816Z","iopub.execute_input":"2024-03-12T14:04:14.523803Z","iopub.status.idle":"2024-03-12T14:04:17.443668Z","shell.execute_reply.started":"2024-03-12T14:04:14.523764Z","shell.execute_reply":"2024-03-12T14:04:17.442482Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### End","metadata":{"id":"QHcY1XR9k4qG"}}]}