{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:51.435671Z","iopub.status.busy":"2024-03-10T03:46:51.434958Z","iopub.status.idle":"2024-03-10T03:46:51.447220Z","shell.execute_reply":"2024-03-10T03:46:51.446250Z","shell.execute_reply.started":"2024-03-10T03:46:51.435639Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["nice\n"]}],"source":["print(\"nice\")"]},{"cell_type":"markdown","metadata":{},"source":["### global "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:51.449179Z","iopub.status.busy":"2024-03-10T03:46:51.448900Z","iopub.status.idle":"2024-03-10T03:46:58.091874Z","shell.execute_reply":"2024-03-10T03:46:58.090861Z","shell.execute_reply.started":"2024-03-10T03:46:51.449158Z"},"trusted":true},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import torch \n","print(tf.__version__)\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.095890Z","iopub.status.busy":"2024-03-10T03:46:58.095283Z","iopub.status.idle":"2024-03-10T03:46:58.105202Z","shell.execute_reply":"2024-03-10T03:46:58.104215Z","shell.execute_reply.started":"2024-03-10T03:46:58.095854Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1.24.3\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x1e9f1b3f830>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","print(np.__version__)\n","# setting random_state\n","RANDOM_STATE = 42\n","np.random.seed(RANDOM_STATE)\n","tf.random.set_seed(RANDOM_STATE)\n","torch.manual_seed(RANDOM_STATE)"]},{"cell_type":"markdown","metadata":{},"source":["### some libraries and functions "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.107279Z","iopub.status.busy":"2024-03-10T03:46:58.106432Z","iopub.status.idle":"2024-03-10T03:46:58.452054Z","shell.execute_reply":"2024-03-10T03:46:58.451227Z","shell.execute_reply.started":"2024-03-10T03:46:58.107253Z"},"trusted":true},"outputs":[],"source":["# libraries\n","import sys, math \n","from collections import defaultdict\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import sklearn"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.454862Z","iopub.status.busy":"2024-03-10T03:46:58.454295Z","iopub.status.idle":"2024-03-10T03:46:58.462716Z","shell.execute_reply":"2024-03-10T03:46:58.461718Z","shell.execute_reply.started":"2024-03-10T03:46:58.454817Z"},"trusted":true},"outputs":[],"source":["# fix random_state\n","def fixRandomState(fixed_state: int=RANDOM_STATE):\n","  np.random.seed(fixed_state)\n","  tf.random.set_seed(fixed_state)\n","  torch.manual_seed(fixed_state)\n","  \n","# exception\n","def exception(requirement: bool, content):\n","  if(requirement == False): raise ValueError(content)\n","def catchException(ex: Exception):\n","  print(type(ex), ex.args)\n","  exception(False, ex)\n","\n","# message\n","def mesVerbose(flag: bool, verbose, func_dir: str=\"\"):\n","  if(flag == False): return\n","  print(\"__verbose__:\", func_dir, verbose)\n","def mesWarning(note, func_dir: str=\"\"):\n","  print(\"__warning__:\", func_dir, str(note) + \"@@@\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.464382Z","iopub.status.busy":"2024-03-10T03:46:58.464053Z","iopub.status.idle":"2024-03-10T03:46:58.472781Z","shell.execute_reply":"2024-03-10T03:46:58.471931Z","shell.execute_reply.started":"2024-03-10T03:46:58.464349Z"},"trusted":true},"outputs":[],"source":["def over(val, name=\"\") -> tuple:\n","  try: mesVerbose(True, (type(val), val.shape, str(sys.getsizeof(val)) + \"Bytes\"), name)\n","  except: mesVerbose(True, (type(val), \"no shape\", str(sys.getsizeof(val)) + \"Bytes\"), name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ModelMes: \n","  FLAG = True \n","  def __init__(self): \n","    pass\n","  def mesIn(in_shape, nameclass=\"\"): \n","    mesVerbose(ModelMes.FLAG, in_shape, nameclass + \": in_shape=\")\n","  def mesOut(in_shape, nameclass=\"\"): \n","    mesVerbose(ModelMes.FLAG, in_shape, nameclass.upper() + \"> __init__: out_shape=\")\n","    ModelMes.FLAG = True \n","  def getOUT_SHAPE(model): \n","    return model.OUT_SHAPE"]},{"cell_type":"markdown","metadata":{},"source":["### model architecture "]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.485961Z","iopub.status.busy":"2024-03-10T03:46:58.485664Z","iopub.status.idle":"2024-03-10T03:46:58.493427Z","shell.execute_reply":"2024-03-10T03:46:58.492461Z","shell.execute_reply.started":"2024-03-10T03:46:58.485938Z"},"trusted":true},"outputs":[],"source":["from torch import nn, optim\n","from torch.utils import data \n","\n","INPUT_SHAPE = (224, 224, 3)\n","\n","YOLO_BACKBONE_ARCHITECTURE = [(64, 7, 2, 'same'), 'M',\n","                                (192, 3, 1, 'same'), 'M',\n","                                (128, 1, 1, 'valid'),\n","                                [(128, 256), 1],\n","                                [(256, 512), 1], 'M',\n","                                [(256, 512), 4],\n","                                [(512, 1024), 1], 'M',\n","                                [(512, 1024), 2]]\n","\n","GRID_SIZE = 7\n","NUM_BOXES = 2\n","NUM_CLASSES = 3 "]},{"cell_type":"markdown","metadata":{},"source":["##### blcoks"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.495263Z","iopub.status.busy":"2024-03-10T03:46:58.494890Z","iopub.status.idle":"2024-03-10T03:46:58.506457Z","shell.execute_reply":"2024-03-10T03:46:58.505472Z","shell.execute_reply.started":"2024-03-10T03:46:58.495210Z"},"trusted":true},"outputs":[],"source":["class ConvWithBatchNorm(nn.Module):\n","  \"\"\"Conv layer with batch norm and leaky relu\"\"\"\n","  \n","  def __init__(self, in_c: int, out_c: int, k_size: int, stride=1, padding='same', negative_slope=0.1):\n","    super().__init__()\n","    \n","    if(stride == 2): padding = 0 \n","    layers = [nn.Conv2d(in_c, out_c, k_size, stride=stride, padding=padding, bias=False)]\n","    layers += [nn.BatchNorm2d(num_features=out_c)]\n","    layers += [nn.LeakyReLU(negative_slope=negative_slope)]\n","    self.layers = layers \n","  \n","  def forward(self, x): \n","    for layer in self.layers: \n","      x = layer(x) \n","    return x "]},{"cell_type":"code","execution_count":141,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.507903Z","iopub.status.busy":"2024-03-10T03:46:58.507592Z","iopub.status.idle":"2024-03-10T03:46:58.515625Z","shell.execute_reply":"2024-03-10T03:46:58.514681Z","shell.execute_reply.started":"2024-03-10T03:46:58.507874Z"},"trusted":true},"outputs":[],"source":["class BottleNeckBlock(nn.Module):\n","  \"\"\"Block of 1x1 reduction layers followed by 3x3 conv. layer\"\"\"\n","  \n","  def __init__(self, in_c: int, out_ces: tuple, num_repeat: int):\n","    super().__init__() \n","    \n","    out_1x1 = out_ces[0]\n","    out_3x3 = out_ces[1] \n","    layers = [] \n","    for i in range(num_repeat): \n","      layers += [nn.Conv2d(in_c, out_1x1, 1, stride=1, padding='same', bias=False)]\n","      layers += [nn.Conv2d(out_1x1, out_3x3, 3, stride=1, padding='same', bias=False)]\n","    self.layers = layers \n","    \n","  def forward(self, x): \n","    for layer in self.layers: \n","      x = layer(x) \n","    return x "]},{"cell_type":"markdown","metadata":{},"source":["##### YoloBackbone"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.519558Z","iopub.status.busy":"2024-03-10T03:46:58.519284Z","iopub.status.idle":"2024-03-10T03:46:58.528646Z","shell.execute_reply":"2024-03-10T03:46:58.527761Z","shell.execute_reply.started":"2024-03-10T03:46:58.519535Z"},"trusted":true},"outputs":[],"source":["class YoloBackbone(nn.Module):\n","  \"\"\"YOLO backbone extract feature from the input\"\"\"\n","\n","  def __init__(self, in_shape: tuple=INPUT_SHAPE, backbone_config=YOLO_BACKBONE_ARCHITECTURE, mes_flag = False):\n","    super().__init__()\n","    ModelMes.FLAG = mes_flag \n","    \n","    model = []  \n","    in_shape = list(INPUT_SHAPE) \n","    # print(in_shape) \n","    for i, config in enumerate(backbone_config):\n","      # print(in_shape) \n","      ModelMes.mesIn(in_shape, \"YoloBackbone\") \n","      if type(config) == tuple:\n","        out_c, k_size, stride, padding = config\n","        model += [ConvWithBatchNorm(in_c=in_shape[2], out_c=out_c, k_size=k_size, stride=stride, padding=padding, negative_slope=0.1)] \n","        in_shape = [in_shape[0] // stride, in_shape[1] // stride, out_c] \n","        \n","      elif type(config) == str:\n","        model += [nn.MaxPool2d(kernel_size=2, stride=2, padding='same')]  \n","        in_shape[0], in_shape[1] = in_shape[0]//2, in_shape[1]//2\n","        \n","      elif type(config) == list:\n","        out_ces, num_repeat = config\n","        model += [BottleNeckBlock(in_shape[2], out_ces, num_repeat)] \n","        in_shape[2] = out_ces[1] \n","    \n","    ModelMes.mesOut(in_shape, \"YoloBackbone\")\n","    self.model = model\n","    self.OUT_SHAPE = in_shape\n","    \n","  def forward(self, x): \n","    for layer in self.model:  \n","      x = layer(x) \n","    return x "]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["YoloBackbone() "]},{"cell_type":"markdown","metadata":{},"source":["##### YoloOutput "]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":["YOLO_OUT_ARCHITECTURE = [(4096, 0.1), 0.5, (2040, 0.1), 0.5, (1024, 0.1), 0.5, (GRID_SIZE * GRID_SIZE * (NUM_BOXES * 5 + NUM_CLASSES), 0.1)]"]},{"cell_type":"code","execution_count":186,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.943891Z","iopub.status.busy":"2024-03-10T03:46:58.943518Z","iopub.status.idle":"2024-03-10T03:46:58.954778Z","shell.execute_reply":"2024-03-10T03:46:58.953883Z","shell.execute_reply.started":"2024-03-10T03:46:58.943858Z"},"trusted":true},"outputs":[],"source":["class YoloOutput(nn.Module):\n","  \"\"\"YOLO last convolution and FC layers to produce prediction\"\"\"\n","\n","  def __init__(self, in_shape: tuple):\n","    super().__init__() \n","    in_shape = list(in_shape) \n","    mesVerbose(True, in_shape, \"YoloOutput: \" + \"in_shape=\")\n","    model = [ConvWithBatchNorm(in_shape[2], out_c=1024, k_size=3),\n","              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n","              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n","              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n","              nn.Flatten()] \n","    in_shape = int(in_shape[0] * in_shape[1] * 1024)\n","    \n","    for i, config in enumerate(YOLO_OUT_ARCHITECTURE): \n","      mesVerbose(True, in_shape, \"YoloOutput: \" + \"in_shape=\")\n","      if type(config) == tuple: \n","        out_f, slop = config \n","        model += [nn.Linear(in_features=in_shape, out_features=out_f), nn.LeakyReLU(negative_slope=slop)]\n","        in_shape = out_f \n","        \n","      else: \n","        p = config \n","        model += [nn.Dropout(p=0.5)] \n","\n","    mesVerbose(True, in_shape, \"YoloOutput > __init__: \" + \"out_shape=\")\n","    self.model = model\n","    self.OUT_SHAPE = in_shape\n","  \n","  def forward(self, x): \n","    for layer in self.model:  \n","      x = layer(x) \n","    return x "]},{"cell_type":"code","execution_count":174,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:58.956317Z","iopub.status.busy":"2024-03-10T03:46:58.956005Z","iopub.status.idle":"2024-03-10T03:46:59.677473Z","shell.execute_reply":"2024-03-10T03:46:59.676383Z","shell.execute_reply.started":"2024-03-10T03:46:58.956292Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: YoloBackbone: in_shape= [224, 224, 3]\n","__verbose__: YoloBackbone: in_shape= [112, 112, 64]\n","__verbose__: YoloBackbone: in_shape= [56, 56, 64]\n","__verbose__: YoloBackbone: in_shape= [56, 56, 192]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 192]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 128]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 256]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 1024]\n","__verbose__: YoloBackbone: in_shape= [7, 7, 1024]\n","__verbose__: YoloBackbone >__init__: out_shape= [7, 7, 1024]\n","__verbose__: YoloOutput: in_shape= [7, 7, 1024]\n","__verbose__: YoloOutput: in_shape= 50176\n","__verbose__: YoloOutput: in_shape= 4096\n","__verbose__: YoloOutput: in_shape= 4096\n","__verbose__: YoloOutput: in_shape= 2040\n","__verbose__: YoloOutput: in_shape= 2040\n","__verbose__: YoloOutput: in_shape= 1024\n","__verbose__: YoloOutput: in_shape= 1024\n","__verbose__: YoloOutput > __init__: out_shape= 637\n","YoloOutput()\n"]}],"source":["print(YoloOutput(YoloBackbone().out_shape))"]},{"cell_type":"markdown","metadata":{},"source":["##### YoloV1 "]},{"cell_type":"code","execution_count":198,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:59.980516Z","iopub.status.busy":"2024-03-10T03:46:59.979894Z","iopub.status.idle":"2024-03-10T03:46:59.989798Z","shell.execute_reply":"2024-03-10T03:46:59.988801Z","shell.execute_reply.started":"2024-03-10T03:46:59.980480Z"},"trusted":true},"outputs":[],"source":["class YoloV1(nn.Module):\n","  \"\"\"End-to-end YOLO network\"\"\"\n","\n","  def __init__(self, in_shape=INPUT_SHAPE): \n","    super().__init__() \n","    \n","    in_shape = list(in_shape)\n","    ModelMes.mesIn(in_shape=in_shape, nameclass=\"YoloV1\") \n","    yolo_backbone = YoloBackbone()\n","    in_shape = ModelMes.getOUT_SHAPE(model=yolo_backbone)\n","    \n","    ModelMes.mesIn(in_shape=in_shape, nameclass=\"YoloV1\") \n","    yolo_output = YoloOutput(in_shape=in_shape) \n","    in_shape = ModelMes.getOUT_SHAPE(model=yolo_output) \n","    \n","    ModelMes.mesOut(in_shape=in_shape, nameclass=\"YoloV1\")\n","    self.model = [yolo_backbone, yolo_output]\n","    self.OUT_SHAPE = in_shape \n","    \n","  def forward(self, x): \n","    for layer in self.model:  \n","      x = layer(x) \n","    return x \n","    "]},{"cell_type":"code","execution_count":199,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: YoloV1: in_shape= [224, 224, 3]\n","__verbose__: YoloBackbone: in_shape= [224, 224, 3]\n","__verbose__: YoloBackbone: in_shape= [112, 112, 64]\n","__verbose__: YoloBackbone: in_shape= [56, 56, 64]\n","__verbose__: YoloBackbone: in_shape= [56, 56, 192]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 192]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 128]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 256]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 1024]\n","__verbose__: YoloBackbone: in_shape= [7, 7, 1024]\n","__verbose__: YoloBackbone >__init__: out_shape= [7, 7, 1024]\n","__verbose__: YoloV1: in_shape= [7, 7, 1024]\n","__verbose__: YoloOutput: in_shape= [7, 7, 1024]\n","__verbose__: YoloOutput: in_shape= 50176\n","__verbose__: YoloOutput: in_shape= 4096\n","__verbose__: YoloOutput: in_shape= 4096\n","__verbose__: YoloOutput: in_shape= 2040\n","__verbose__: YoloOutput: in_shape= 2040\n","__verbose__: YoloOutput: in_shape= 1024\n","__verbose__: YoloOutput: in_shape= 1024\n","__verbose__: YoloOutput > __init__: out_shape= 637\n","__verbose__: YOLOV1> __init__: out_shape= 637\n"]},{"data":{"text/plain":["YoloV1()"]},"execution_count":199,"metadata":{},"output_type":"execute_result"}],"source":["YoloV1() "]},{"cell_type":"markdown","metadata":{},"source":["### YoloLoss "]},{"cell_type":"code","execution_count":200,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:46:59.991771Z","iopub.status.busy":"2024-03-10T03:46:59.991405Z","iopub.status.idle":"2024-03-10T03:47:00.014653Z","shell.execute_reply":"2024-03-10T03:47:00.013718Z","shell.execute_reply.started":"2024-03-10T03:46:59.991737Z"},"trusted":true},"outputs":[],"source":["class YoloLoss(nn.Module): \n","  def __init__(self, coord_c=5, noobj_c=0.5): \n","    super().__init__()\n","    self.COORD = coord_c\n","    self.NOOBJ = noobj_c \n","  \n","  def expDim(self, val, unsque): \n","    return torch.tensor(val, dtype=torch.float32).unsqueeze(unsque)\n","    \n","  def computeArea(self, a, b, c, d): \n","    return (c-a) * (d-b) \n","  def computeIOU(self, cell, cell0): \n","    x, y, w, h = cell[..., [0]], cell[..., [1]], cell[..., [2]], cell[..., [3]] \n","    x0, y0, w0, h0 = cell0[..., [0]], cell0[..., [1]], cell0[..., [2]], cell0[..., [3]]  \n","    a, b, c, d = torch.min(x-w, x0-w0), torch.min(y-h, y0-h0), torch.max(x+w, x0+w0), torch.max(y+h, y0+h0)\n","    return self.computeArea(a, b, c, d) / (self.computeArea(x-w, y-h, x+w, y+h) + self.computeArea(x0-w0, y0-h0, x0+w0, y0+w0) - self.computeArea(a, b, c, d))\n","  \n","  def sqrt_sign(self, x): \n","    return torch.where(x >= 0, torch.sqrt(x), -torch.sqrt(-x))\n","  def computeCoorLoss(self, O, mask0, cell, cell0): \n","    x, y, w, h = cell[..., [0]], cell[..., [1]], cell[..., [2]], cell[..., [3]] \n","    x0, y0, w0, h0 = cell0[..., [0]], cell0[..., [1]], cell0[..., [2]], cell0[..., [3]] \n","#     over(mask0) \n","    loss1 = torch.sum(O * mask0 * ((x-x0)**2 + (y-y0)**2))\n","    w, h, w0, h0 = self.sqrt_sign(w), self.sqrt_sign(h), self.sqrt_sign(w0), self.sqrt_sign(h0)  \n","    loss2 = torch.sum(O * mask0 * ((w-w0)**2 + (h-h0)**2)) \n","    return loss1 + loss2 \n","  \n","  def forward(self, predictions: torch.Tensor, target: torch.Tensor) -> torch.Tensor: \n","    y_true, y_pred = target, predictions \n","    cell = y_true[..., :4] # (N, S, S, 4)\n","    O = y_true[..., [4]]\n","    # O = self.expDim(y_true[..., 4], 3) # (N, S, S, 1) \n","    P = y_true[..., 5:] # (N, S, S, C)\n","    N = y_true.shape[0] \n","\n","    cell1 = y_pred[..., :4]\n","    O1 = y_pred[..., [4]]\n","    # O1 = self.expDim(y_pred[..., 4], 3) \n","    cell2 = y_pred[..., 5:9]\n","    O2 = y_pred[..., [9]]\n","    # O2 = self.expDim(y_pred[..., 9], 3) \n","    P0 = y_pred[..., 10:]\n","    \n","#     print(O, P, P0) \n","    over(y_pred)\n","    print(torch.min(y_pred), torch.max(y_pred))\n","    classloss = torch.sum(O * (P - P0)**2)  \n","    print(classloss) \n","    \n","    iou1 = self.computeIOU(cell=cell, cell0=cell1)\n","    iou2 = self.computeIOU(cell=cell, cell0=cell2) \n","    mask1 = torch.where(iou1 > iou2, 1, 0)\n","    mask2 = torch.where(iou1 < iou2, 1, 0) \n","    coorloss = self.computeCoorLoss(O, mask1, cell, cell1) + self.computeCoorLoss(O, mask2, cell, cell2) \n","    coorloss *= self.COORD    \n","    print(coorloss) \n","    \n","    objloss = torch.sum(O * mask1 * (O-O1)**2) + torch.sum(O * mask2 * (O-O2)**2)\n","    print(objloss)\n","    \n","    noobjloss = torch.sum((1-O) * (O-O1)**2 + (1-O) * (O-O2)**2)\n","    noobjloss *= self.NOOBJ \n","    print(noobjloss) \n","    return (classloss + coorloss + objloss + noobjloss) / N \n","  "]},{"cell_type":"code","execution_count":201,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:00.016141Z","iopub.status.busy":"2024-03-10T03:47:00.015793Z","iopub.status.idle":"2024-03-10T03:47:00.025351Z","shell.execute_reply":"2024-03-10T03:47:00.024521Z","shell.execute_reply.started":"2024-03-10T03:47:00.016112Z"},"trusted":true},"outputs":[],"source":["DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":203,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:00.026680Z","iopub.status.busy":"2024-03-10T03:47:00.026374Z","iopub.status.idle":"2024-03-10T03:47:00.840781Z","shell.execute_reply":"2024-03-10T03:47:00.839556Z","shell.execute_reply.started":"2024-03-10T03:47:00.026609Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__:  (<class 'torch.Tensor'>, torch.Size([224, 224, 3]), '88Bytes')\n","__verbose__: YoloV1: in_shape= [224, 224, 3]\n","__verbose__: YoloBackbone: in_shape= [224, 224, 3]\n","__verbose__: YoloBackbone: in_shape= [112, 112, 64]\n","__verbose__: YoloBackbone: in_shape= [56, 56, 64]\n","__verbose__: YoloBackbone: in_shape= [56, 56, 192]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 192]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 128]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 256]\n","__verbose__: YoloBackbone: in_shape= [28, 28, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 512]\n","__verbose__: YoloBackbone: in_shape= [14, 14, 1024]\n","__verbose__: YoloBackbone: in_shape= [7, 7, 1024]\n","__verbose__: YoloBackbone >__init__: out_shape= [7, 7, 1024]\n","__verbose__: YoloV1: in_shape= [7, 7, 1024]\n","__verbose__: YoloOutput: in_shape= [7, 7, 1024]\n","__verbose__: YoloOutput: in_shape= 50176\n","__verbose__: YoloOutput: in_shape= 4096\n","__verbose__: YoloOutput: in_shape= 4096\n","__verbose__: YoloOutput: in_shape= 2040\n","__verbose__: YoloOutput: in_shape= 2040\n","__verbose__: YoloOutput: in_shape= 1024\n","__verbose__: YoloOutput: in_shape= 1024\n","__verbose__: YoloOutput > __init__: out_shape= 637\n","__verbose__: YOLOV1> __init__: out_shape= 637\n"]},{"ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 224, 224, 3] to have 3 channels, but got 224 channels instead","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[203], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m over(imgs) \n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YoloV1()\n\u001b[1;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      5\u001b[0m over(out) \n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[198], line 22\u001b[0m, in \u001b[0;36mYoloV1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[0;32m     21\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:  \n\u001b[1;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[185], line 33\u001b[0m, in \u001b[0;36mYoloBackbone.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[0;32m     32\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:  \n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     34\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[140], line 15\u001b[0m, in \u001b[0;36mConvWithBatchNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[0;32m     14\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: \n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 224, 224, 3] to have 3 channels, but got 224 channels instead"]}],"source":["imgs = torch.rand(224, 224, 3)\n","over(imgs) \n","model = YoloV1()\n","out = model(imgs) \n","over(out) "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:00.842523Z","iopub.status.busy":"2024-03-10T03:47:00.842145Z","iopub.status.idle":"2024-03-10T03:47:01.075822Z","shell.execute_reply":"2024-03-10T03:47:01.074914Z","shell.execute_reply.started":"2024-03-10T03:47:00.842489Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: over: (<class 'torch.Tensor'>, torch.Size([16, 7, 7, 25]), '80Bytes')\n","__verbose__: over: (<class 'torch.Tensor'>, torch.Size([16, 7, 7, 30]), '80Bytes')\n","tensor(-1.6802e-05, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.9366e-05, device='cuda:0', grad_fn=<MaxBackward1>)\n","tensor(2591.5640, device='cuda:0', grad_fn=<SumBackward0>)\n","tensor(2464.1858, device='cuda:0', grad_fn=<MulBackward0>)\n","tensor(146.7066, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(64.8907, device='cuda:0', grad_fn=<MulBackward0>)\n"]},{"data":{"text/plain":["tensor(329.2092, device='cuda:0', grad_fn=<DivBackward0>)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["out_true = torch.rand(16, 7, 7, 25).to(DEVICE) \n","over(out_true) \n","# print(out_true)\n","loss = YoloLoss()\n","loss.call(y_pred=out, y_true=out_true) "]},{"cell_type":"markdown","metadata":{},"source":["### DataLoad"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:01.077602Z","iopub.status.busy":"2024-03-10T03:47:01.077296Z","iopub.status.idle":"2024-03-10T03:47:01.112647Z","shell.execute_reply":"2024-03-10T03:47:01.111724Z","shell.execute_reply.started":"2024-03-10T03:47:01.077577Z"},"trusted":true},"outputs":[],"source":["import os\n","from xml.etree import ElementTree\n","import tensorflow as tf\n","from tqdm import tqdm\n","from functools import partial\n","from keras.preprocessing.image import load_img, img_to_array\n","from torch.utils.data import Dataset, DataLoader\n","\n","class_names = ['apple', 'banana', 'orange']\n","\n","class DataLoad(Dataset): \n","  def __init__(self, file_dir, input_shape, grid_size=7) -> None:\n","    super().__init__ \n","    dataframe = self.get_dataframe(file_dir=file_dir)\n","    self.imgs, self.labels = self.load_dataset(dataframe, input_shape, grid_size) # np.ndarray \n","  \n","  def __len__(self):\n","    return len(self.imgs)\n","\n","  def __getitem__(self, idx): #!!! get data \n","      x, y = self.imgs[idx], self.labels[idx] # np.ndarray \n","      x, y = tf.convert_to_tensor(x), tf.convert_to_tensor(y) # tf.tensor \n","      x, y = self._apply_augmentation(x, y, seed=RANDOM_STATE) # tf.tensor \n","      # cast type \n","      x = torch.tensor(x.numpy(), dtype=torch.float32)  # torch.tensor \n","      y = torch.tensor(y.numpy(), dtype=torch.float32)\n","      # y = torch.tensor(y.numpy(), dtype=torch.float32).unsqueeze(0) over into [N, 1, ...]\n","      x, y = x.to(DEVICE), y.to(DEVICE) # torch.tensor.device \n","#       x, y = self._apply_augmentation(x, y, seed=RANDOM_STATE) # tf.tensor \n","      # over(x)\n","      # over(y) \n","      return x, y\n","  \n","  \n","  def get_dataframe(self, file_dir):\n","    \"\"\"\n","    Get the train/val/test dataframe which contains image\n","    file names and annotations files. If `phase = train',\n","    return train and val set\n","    :param file_dir: File directory to create dataframe\n","    :return file_df: Train or test dataframe\n","    \"\"\"\n","\n","    img_files = [os.path.join(file_dir, img_file) for img_file\n","                 in sorted(os.listdir(file_dir)) if img_file[-4:] == '.jpg']\n","    annot_files = [img_file[:-4] + '.xml' for img_file in img_files]\n","\n","    img_file_series = pd.Series(img_files, name='Image_file')\n","    annot_file_series = pd.Series(annot_files, name='Annotation_file')\n","    file_df = pd.DataFrame(pd.concat([img_file_series, annot_file_series], axis=1))\n","\n","    return file_df\n","\n","  def prepare_image(self, filename, input_shape):\n","    \"\"\"\n","    Resize image to expected dimension, and opt. apply some random transformation.\n","    :param filename: File name\n","    :param input_shape: Shape expected by the model (image will be resize accordingly)\n","    :return : 3D image array, pixel values from [0., 1.]\n","    \"\"\"\n","\n","    img = img_to_array(load_img(filename, target_size=input_shape)) / 255.\n","\n","    return img\n","\n","  def convert_to_xywh(self, bboxes):\n","    \"\"\"\n","    Convert list of (xmin, ymin, xmax, ymax) to\n","    (x_center, y_center, box_width, box_height)\n","    :param bboxes: List of bounding boxes, each has 4\n","    values (xmin, ymin, xmax, ymax)\n","    :return boxes: List of bounding boxes, each has 4\n","    values (x_center, y_center, box_width, box_height)\n","    \"\"\"\n","\n","    boxes = list()\n","    for box in bboxes:\n","        xmin, ymin, xmax, ymax = box\n","\n","        # Compute width and height of box\n","        box_width = xmax - xmin\n","        box_height = ymax - ymin\n","\n","        # Compute x, y center\n","        x_center = int(xmin + (box_width / 2))\n","        y_center = int(ymin + (box_height / 2))\n","\n","        boxes.append((x_center, y_center, box_width, box_height))\n","\n","    return boxes\n","\n","  def extract_annotation_file(self, filename):\n","    \"\"\"\n","    Extract bounding boxes from an annotation file\n","    :param filename: Annotation file name\n","    :return boxes: List of bounding boxes in image, each box has\n","    4 values (x_center, y_center, box_width, box_height)\n","    :return classes: List of classes in image\n","    :return width: Width of image\n","    :return height: Height of image\n","    \"\"\"\n","\n","    # Load and parse the file\n","    tree = ElementTree.parse(filename)\n","    # Get the root of the document\n","    root = tree.getroot()\n","    boxes = list()\n","    classes = list()\n","\n","    # Extract each bounding box\n","    for box in root.findall('.//object'):\n","        cls = class_names.index(box.find('name').text)\n","        xmin = int(box.find('bndbox/xmin').text)\n","        ymin = int(box.find('bndbox/ymin').text)\n","        xmax = int(box.find('bndbox/xmax').text)\n","        ymax = int(box.find('bndbox/ymax').text)\n","        coors = (xmin, ymin, xmax, ymax)\n","        boxes.append(coors)\n","        classes.append(cls)\n","\n","    boxes = self.convert_to_xywh(boxes)\n","\n","    # Get width and height of an image\n","    width = int(root.find('.//size/width').text)\n","    height = int(root.find('.//size/height').text)\n","\n","    # Some annotation files have set width and height by 0,\n","    # so we need to load image and get it width and height\n","    if (width == 0) or (height == 0):\n","        img = load_img(filename[:-4] + '.jpg')\n","        width, height = img.width, img.height\n","\n","    return boxes, classes, width, height\n","\n","  def convert_bboxes_to_tensor(self, bboxes, classes, img_width, img_height, grid_size=7):\n","    \"\"\"\n","    Convert list of bounding boxes to tensor target\n","    :param bboxes: List of bounding boxes in image, each box has\n","    4 values (x_center, y_center, box_width, box_height)\n","    :param classes: List of class in image\n","    :param img_width: Image's width\n","    :param img_height: Image's height\n","    :param grid_size: Grid size\n","    :return target: Target tensor (grid_size x grid_size x (5 + num_classes))\n","    \"\"\"\n","\n","    num_classes = len(class_names)\n","    target = np.zeros(shape=(grid_size, grid_size, 5 + num_classes), dtype=np.float32)\n","\n","    for idx, bbox in enumerate(bboxes):\n","        x_center, y_center, width, height = bbox\n","\n","        # Compute size of each cell in grid\n","        cell_w, cell_h = img_width / grid_size, img_height / grid_size\n","\n","        # Determine cell i, j of bounding box\n","        i, j = int(y_center / cell_h), int(x_center / cell_w)\n","\n","        # Compute value of x_center and y_center in cell\n","        x, y = (x_center / cell_w) - j, (y_center / cell_h) - i\n","\n","        # Normalize width and height of bounding box\n","        w_norm, h_norm = width / img_width, height / img_height\n","\n","        # Add bounding box to tensor\n","        # Set x, y, w, h\n","        target[i, j, :4] += (x, y, w_norm, h_norm)\n","        # Set obj score\n","        target[i, j, 4] = 1.\n","        # Set class dist.\n","        target[i, j, 5 + classes[idx]] = 1.\n","    \n","#     over(target) \n","    return target\n","\n","  def load_dataset(self, dataframe, input_shape, grid_size=7):\n","    \"\"\"\n","    Load img and target tensor\n","    :param dataframe: Dataframe contains img files and annotation files\n","    :param input_shape: Shape expected by the model (image will be resize accordingly)\n","    :param grid_size: Grid size\n","    :return dataset: Iterable dataset\n","    \"\"\"\n","\n","    imgs, targets = list(), list()\n","\n","    for _, row in tqdm(dataframe.iterrows()):\n","        img = self.prepare_image(row.Image_file, input_shape)\n","        target = self.extract_annotation_file(row.Annotation_file)\n","        target = self.convert_bboxes_to_tensor(*target, grid_size)\n","        imgs.append(img)\n","        targets.append(target)\n","\n","    imgs = np.array(imgs)\n","    targets = np.array(targets)\n","    return imgs, targets \n","  \n","    # dataset = tf.data.Dataset.from_tensor_slices((imgs, targets))\n","    # return dataset\n","\n","\n","  def _apply_augmentation(self, image, target, seed=None):\n","    \"\"\"\n","    Apply random brightness and saturation on image\n","    :param image: Image to augment\n","    :param target: Target tensor\n","    :param seed: Seed for random operation\n","    :return : Processed data\n","    \"\"\"\n","\n","    # Random bright & saturation change\n","    image = tf.image.random_brightness(image, max_delta=0.1, seed=seed)\n","    image = tf.image.random_saturation(image, lower=0.5, upper=1.5, seed=seed)\n","\n","    # Keeping pixel values in check\n","    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n","\n","    return image, target\n","\n","\n","  def load_dataset_from_df(self, dataframe, batch_size=32, num_repeat=None, shuffle=False,\n","                         input_shape=(448, 448, 3), grid_size=7, augment=False,\n","                         seed=None):\n","    \"\"\"\n","    Instantiate dataset\n","    :param dataframe: Dataframe contains img files and annotation files\n","    :param batch_size: Batch size\n","    :param num_epochs: Number of epochs (to repeat the iteration - infinite if None)\n","    :param shuffle: Flag to shuffle the dataset (if True)\n","    :param input_shape: Shape of the processed image\n","    :param grid_size: Grid size\n","    :param augment: Flag to apply some random augmentations to the image\n","    :param seed: Random seed for operation\n","    :return : Iterable dataset\n","    \"\"\"\n","\n","    apply_augmentation = partial(self._apply_augmentation, seed=seed)\n","\n","    dataset = self.load_dataset(dataframe, input_shape, grid_size)\n","    ### !!!\n","    dataset = dataset.repeat(num_repeat)\n","    if shuffle:\n","        dataset = dataset.shuffle(1000, seed)\n","    if augment:\n","        dataset = dataset.map(apply_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:01.114229Z","iopub.status.busy":"2024-03-10T03:47:01.113825Z","iopub.status.idle":"2024-03-10T03:47:01.127777Z","shell.execute_reply":"2024-03-10T03:47:01.126753Z","shell.execute_reply.started":"2024-03-10T03:47:01.114201Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(224, 224, 3)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["INPUT_SHAPE "]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:01.129581Z","iopub.status.busy":"2024-03-10T03:47:01.129181Z","iopub.status.idle":"2024-03-10T03:47:04.085670Z","shell.execute_reply":"2024-03-10T03:47:04.084832Z","shell.execute_reply.started":"2024-03-10T03:47:01.129547Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["26it [00:00, 126.25it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","240it [00:02, 83.12it/s] \n"]}],"source":["train_dir = '/kaggle/input/dataset/fruits_dataset/train'\n","input_shape = INPUT_SHAPE \n","grid_size = 7\n","num_repeat = 2\n","batch_size = 16\n","dataload = DataLoad(train_dir, input_shape=INPUT_SHAPE, grid_size=grid_size) \n","train_df = dataload.get_dataframe(train_dir)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:04.087088Z","iopub.status.busy":"2024-03-10T03:47:04.086785Z","iopub.status.idle":"2024-03-10T03:47:04.092216Z","shell.execute_reply":"2024-03-10T03:47:04.091248Z","shell.execute_reply.started":"2024-03-10T03:47:04.087063Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: over: (<class '__main__.DataLoad'>, 'no shape', '48Bytes')\n"]}],"source":["over(dataload) "]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:04.093575Z","iopub.status.busy":"2024-03-10T03:47:04.093285Z","iopub.status.idle":"2024-03-10T03:47:05.154391Z","shell.execute_reply":"2024-03-10T03:47:05.153377Z","shell.execute_reply.started":"2024-03-10T03:47:04.093541Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: over: (<class 'torch.Tensor'>, torch.Size([224, 224, 3]), '80Bytes')\n"]}],"source":["over(dataload[0][0]) "]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.156054Z","iopub.status.busy":"2024-03-10T03:47:05.155708Z","iopub.status.idle":"2024-03-10T03:47:05.167732Z","shell.execute_reply":"2024-03-10T03:47:05.166574Z","shell.execute_reply.started":"2024-03-10T03:47:05.156026Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: over: (<class 'torch.Tensor'>, torch.Size([2, 7, 7, 8]), '80Bytes')\n"]}],"source":["over(dataload[0:2][1])"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.169394Z","iopub.status.busy":"2024-03-10T03:47:05.169010Z","iopub.status.idle":"2024-03-10T03:47:05.173886Z","shell.execute_reply":"2024-03-10T03:47:05.172894Z","shell.execute_reply.started":"2024-03-10T03:47:05.169351Z"},"trusted":true},"outputs":[],"source":["# Assuming train_dataset is your training dataset\n","# train_loader = DataLoader(dataset=dataload, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True, prefetch_factor=2)\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.175464Z","iopub.status.busy":"2024-03-10T03:47:05.175168Z","iopub.status.idle":"2024-03-10T03:47:05.180973Z","shell.execute_reply":"2024-03-10T03:47:05.180101Z","shell.execute_reply.started":"2024-03-10T03:47:05.175440Z"},"trusted":true},"outputs":[],"source":["# Assuming train_dataset is your training dataset\n","train_loader = DataLoader(dataset=dataload, batch_size=16, shuffle=True, drop_last=False)"]},{"cell_type":"markdown","metadata":{},"source":["### training and testing "]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.186588Z","iopub.status.busy":"2024-03-10T03:47:05.186073Z","iopub.status.idle":"2024-03-10T03:47:05.191248Z","shell.execute_reply":"2024-03-10T03:47:05.190237Z","shell.execute_reply.started":"2024-03-10T03:47:05.186564Z"},"trusted":true},"outputs":[],"source":["train_steps_per_epoch = math.ceil(len(train_df) / batch_size)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.192971Z","iopub.status.busy":"2024-03-10T03:47:05.192347Z","iopub.status.idle":"2024-03-10T03:47:05.655731Z","shell.execute_reply":"2024-03-10T03:47:05.654897Z","shell.execute_reply.started":"2024-03-10T03:47:05.192940Z"},"trusted":true},"outputs":[],"source":["yolov1 = YoloV1(input_shape=INPUT_SHAPE, num_classes=3)\n","yolov1.compile(loss=YoloLoss(), optimizer='adam')"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.657123Z","iopub.status.busy":"2024-03-10T03:47:05.656821Z","iopub.status.idle":"2024-03-10T03:47:05.732764Z","shell.execute_reply":"2024-03-10T03:47:05.731822Z","shell.execute_reply.started":"2024-03-10T03:47:05.657099Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: over: (<class 'torch.Tensor'>, torch.Size([16, 7, 7, 13]), '80Bytes')\n"]}],"source":["out = yolov1(dataload[0:16][0])\n","over(out) "]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.734217Z","iopub.status.busy":"2024-03-10T03:47:05.733889Z","iopub.status.idle":"2024-03-10T03:47:05.783697Z","shell.execute_reply":"2024-03-10T03:47:05.782784Z","shell.execute_reply.started":"2024-03-10T03:47:05.734193Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__verbose__: over: (<class 'torch.Tensor'>, torch.Size([16, 7, 7, 13]), '80Bytes')\n","tensor(-1.8650e-05, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.4156e-05, device='cuda:0', grad_fn=<MaxBackward1>)\n","tensor(23.0000, device='cuda:0', grad_fn=<SumBackward0>)\n","tensor(115.0631, device='cuda:0', grad_fn=<MulBackward0>)\n","tensor(14.0001, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(2.7524e-08, device='cuda:0', grad_fn=<MulBackward0>)\n"]}],"source":["out_true = dataload[0:16][1]\n","loss_fn = YoloLoss()\n","loss_vl = loss_fn(y_true=out_true, y_pred=out)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:47:05.785572Z","iopub.status.busy":"2024-03-10T03:47:05.784900Z","iopub.status.idle":"2024-03-10T03:47:05.791998Z","shell.execute_reply":"2024-03-10T03:47:05.791102Z","shell.execute_reply.started":"2024-03-10T03:47:05.785539Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(9.5039, device='cuda:0', grad_fn=<DivBackward0>)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["loss_vl"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:48:45.967884Z","iopub.status.busy":"2024-03-10T03:48:45.967230Z","iopub.status.idle":"2024-03-10T03:48:45.981803Z","shell.execute_reply":"2024-03-10T03:48:45.980885Z","shell.execute_reply.started":"2024-03-10T03:48:45.967829Z"},"trusted":true},"outputs":[],"source":["def train_fn(train_loader, model, optimizer, loss_fn):\n","    loop = tqdm(train_loader, leave=True)\n","    mean_loss = []\n","    \n","    for batch_idx, (x, y) in enumerate(loop):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        over(x)\n","        over(y) \n","        out = model(x)\n","        over(out) \n","        loss = loss_fn(out, y)\n","        print(loss)\n","        mean_loss.append(loss.item())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        loop.set_postfix(loss = loss.item())\n","        \n","    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T03:49:32.743596Z","iopub.status.busy":"2024-03-10T03:49:32.742756Z","iopub.status.idle":"2024-03-10T03:49:32.770236Z","shell.execute_reply":"2024-03-10T03:49:32.769009Z","shell.execute_reply.started":"2024-03-10T03:49:32.743565Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'YoloV1' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYoloV1\u001b[49m(input_shape\u001b[38;5;241m=\u001b[39mINPUT_SHAPE, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)  \n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m      3\u001b[0m         model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m train_fn(train_loader, model, optimizer, loss_fn)\n","\u001b[0;31mNameError\u001b[0m: name 'YoloV1' is not defined"]}],"source":["model = YoloV1(input_shape=INPUT_SHAPE, num_classes=3).to(DEVICE)  \n","optimizer = torch.optim.Adam(\n","        model.parameters(), lr=2e-5, weight_decay=0\n","    )\n","train_fn(train_loader, model, optimizer, loss_fn)"]},{"cell_type":"markdown","metadata":{},"source":["### End "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-10T03:47:06.826275Z","iopub.status.idle":"2024-03-10T03:47:06.826637Z","shell.execute_reply":"2024-03-10T03:47:06.826478Z","shell.execute_reply.started":"2024-03-10T03:47:06.826463Z"},"trusted":true},"outputs":[],"source":["print(hist.history['loss']) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-10T03:47:06.828064Z","iopub.status.idle":"2024-03-10T03:47:06.828397Z","shell.execute_reply":"2024-03-10T03:47:06.828247Z","shell.execute_reply.started":"2024-03-10T03:47:06.828233Z"},"trusted":true},"outputs":[],"source":["from keras.losses import MeanSquaredError\n","l = MeanSquaredError()\n","l(1, 3.4) "]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4569729,"sourceId":7803789,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
