{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:51.435671Z",
          "iopub.status.busy": "2024-03-10T03:46:51.434958Z",
          "iopub.status.idle": "2024-03-10T03:46:51.447220Z",
          "shell.execute_reply": "2024-03-10T03:46:51.446250Z",
          "shell.execute_reply.started": "2024-03-10T03:46:51.435639Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vymEjuKIk4pd",
        "outputId": "b9673206-34bc-4f73-ff1e-42bd692ba2a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nice\n"
          ]
        }
      ],
      "source": [
        "print(\"nice\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMCb_MwNk4pf"
      },
      "source": [
        "### global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:51.449179Z",
          "iopub.status.busy": "2024-03-10T03:46:51.448900Z",
          "iopub.status.idle": "2024-03-10T03:46:58.091874Z",
          "shell.execute_reply": "2024-03-10T03:46:58.090861Z",
          "shell.execute_reply.started": "2024-03-10T03:46:51.449158Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjC16i_mk4pi",
        "outputId": "0c4bf44f-ed0c-4e9b-f43f-8c19db148818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "print(tf.__version__)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.095890Z",
          "iopub.status.busy": "2024-03-10T03:46:58.095283Z",
          "iopub.status.idle": "2024-03-10T03:46:58.105202Z",
          "shell.execute_reply": "2024-03-10T03:46:58.104215Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.095854Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf2_vC6Kk4pj",
        "outputId": "edb4c31a-1a12-499c-a70b-facbb16982cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.25.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ceb7bf4df50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n",
        "# setting random_state\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db9CUa5Ek4pl"
      },
      "source": [
        "### some libraries and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.107279Z",
          "iopub.status.busy": "2024-03-10T03:46:58.106432Z",
          "iopub.status.idle": "2024-03-10T03:46:58.452054Z",
          "shell.execute_reply": "2024-03-10T03:46:58.451227Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.107253Z"
        },
        "trusted": true,
        "id": "r7nwh0e_k4pm"
      },
      "outputs": [],
      "source": [
        "# libraries\n",
        "import sys, math\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.454862Z",
          "iopub.status.busy": "2024-03-10T03:46:58.454295Z",
          "iopub.status.idle": "2024-03-10T03:46:58.462716Z",
          "shell.execute_reply": "2024-03-10T03:46:58.461718Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.454817Z"
        },
        "trusted": true,
        "id": "om06RJL2k4pm"
      },
      "outputs": [],
      "source": [
        "# fix random_state\n",
        "def fixRandomState(fixed_state: int=RANDOM_STATE):\n",
        "  np.random.seed(fixed_state)\n",
        "  tf.random.set_seed(fixed_state)\n",
        "  torch.manual_seed(fixed_state)\n",
        "\n",
        "# exception\n",
        "def exception(requirement: bool, content):\n",
        "  if(requirement == False): raise ValueError(content)\n",
        "def catchException(ex: Exception):\n",
        "  print(type(ex), ex.args)\n",
        "  exception(False, ex)\n",
        "\n",
        "# message\n",
        "def mesVerbose(flag: bool, verbose, func_dir: str=\"\"):\n",
        "  if(flag == False): return\n",
        "  print(\"__verbose__:\", func_dir, verbose)\n",
        "def mesWarning(note, func_dir: str=\"\"):\n",
        "  print(\"__warning__:\", func_dir, str(note) + \"###\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.464382Z",
          "iopub.status.busy": "2024-03-10T03:46:58.464053Z",
          "iopub.status.idle": "2024-03-10T03:46:58.472781Z",
          "shell.execute_reply": "2024-03-10T03:46:58.471931Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.464349Z"
        },
        "trusted": true,
        "id": "1lElqLSLk4pn"
      },
      "outputs": [],
      "source": [
        "def over(val, name=\"\") -> tuple:\n",
        "  try: mesVerbose(True, (type(val), val.shape, str(sys.getsizeof(val)) + \"Bytes\"), name)\n",
        "  except: mesVerbose(True, (type(val), \"no_shape\", str(sys.getsizeof(val)) + \"Bytes\"), name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSU76Uxgk4po"
      },
      "source": [
        "### model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.485961Z",
          "iopub.status.busy": "2024-03-10T03:46:58.485664Z",
          "iopub.status.idle": "2024-03-10T03:46:58.493427Z",
          "shell.execute_reply": "2024-03-10T03:46:58.492461Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.485938Z"
        },
        "trusted": true,
        "id": "LXmFP8FWk4pp"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "from torch.utils import data\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "IN_SHAPE = (BATCH_SIZE, 3, 224, 224)\n",
        "\n",
        "YOLO_BACKBONE_ARCHITECTURE = [(64, 7, 2, 'same'), 'M',\n",
        "                                (192, 3, 1, 'same'), 'M',\n",
        "                                (128, 1, 1, 'valid'),\n",
        "                                [(128, 256), 1],\n",
        "                                [(256, 512), 1], 'M',\n",
        "                                [(256, 512), 4],\n",
        "                                [(512, 1024), 1], 'M',\n",
        "                                [(512, 1024), 2]]\n",
        "\n",
        "GRID_SIZE = 7\n",
        "NUM_BOXES = 2\n",
        "NUM_CLASSES = 3\n",
        "OUT_SHAPE = (16, 7, 7, 13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:00.016141Z",
          "iopub.status.busy": "2024-03-10T03:47:00.015793Z",
          "iopub.status.idle": "2024-03-10T03:47:00.025351Z",
          "shell.execute_reply": "2024-03-10T03:47:00.024521Z",
          "shell.execute_reply.started": "2024-03-10T03:47:00.016112Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SZN5eJek4pv",
        "outputId": "7de2869e-e0db-40d0-e722-b4c7e06f0d3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5MIjFYCk4pp"
      },
      "source": [
        "##### blcoks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.495263Z",
          "iopub.status.busy": "2024-03-10T03:46:58.494890Z",
          "iopub.status.idle": "2024-03-10T03:46:58.506457Z",
          "shell.execute_reply": "2024-03-10T03:46:58.505472Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.495210Z"
        },
        "trusted": true,
        "id": "r5mESj1bk4pq"
      },
      "outputs": [],
      "source": [
        "class ConvWithBatchNorm(nn.Module):\n",
        "  \"\"\"Conv layer with batch norm and leaky relu\"\"\"\n",
        "\n",
        "  def __init__(self, in_c: int, out_c: int, k_size: int, stride=1, negative_slope=0.1):\n",
        "    super(ConvWithBatchNorm, self).__init__()\n",
        "    self.in_shape = ()\n",
        "    self.out_shape = ()\n",
        "\n",
        "    padding = k_size // 2\n",
        "    layers = nn.ModuleList()\n",
        "    layers += [nn.Conv2d(in_c, out_c, k_size, stride=stride, padding=padding, bias=False)]\n",
        "    layers += [nn.BatchNorm2d(num_features=out_c)]\n",
        "    layers += [nn.LeakyReLU(negative_slope=negative_slope)]\n",
        "    self.layers = layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.in_shape = x.shape\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out_shape = x.shape\n",
        "    return x\n",
        "\n",
        "  def getInShape(self): return self.in_shape\n",
        "  def getOutShape(self): return self.out_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.507903Z",
          "iopub.status.busy": "2024-03-10T03:46:58.507592Z",
          "iopub.status.idle": "2024-03-10T03:46:58.515625Z",
          "shell.execute_reply": "2024-03-10T03:46:58.514681Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.507874Z"
        },
        "trusted": true,
        "id": "Nj0k_pkUk4pq"
      },
      "outputs": [],
      "source": [
        "class BottleNeckBlock(nn.Module):\n",
        "  \"\"\"Block of 1x1 reduction layers followed by 3x3 conv. layer\"\"\"\n",
        "\n",
        "  def __init__(self, in_c: int, out_ces: tuple, num_repeat: int):\n",
        "    super(BottleNeckBlock, self).__init__()\n",
        "    self. out_shape = ()\n",
        "\n",
        "    out_1x1 = out_ces[0]\n",
        "    out_3x3 = out_ces[1]\n",
        "    layers = nn.ModuleList()\n",
        "    for i in range(num_repeat):\n",
        "      layers += [nn.Conv2d(in_c, out_1x1, 1, stride=1, padding=0, bias=False)]\n",
        "      layers += [nn.Conv2d(out_1x1, out_3x3, 3, stride=1, padding=1, bias=False)]\n",
        "    self.layers = layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out_shape = x.shape\n",
        "    return x\n",
        "\n",
        "  def getOutShape(self): return self.out_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### nnModule"
      ],
      "metadata": {
        "id": "5bdgFDackvjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QXDFK-vck4pr"
      },
      "outputs": [],
      "source": [
        "class nnModule(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super(nnModule, self).__init__()\n",
        "    self.in_shape = ()\n",
        "    self.out_shape = ()\n",
        "    self.model = nn.ModuleList()\n",
        "\n",
        "  def getInShape(self): return self.in_shape\n",
        "  def getOutShape(self): return self.out_shape\n",
        "  def getModel(self): return self.model\n",
        "  def setInShape(self, in_shape): self.in_shape = in_shape\n",
        "  def setOutShape(self, out_shape): self.out_shape = out_shape\n",
        "  def setModel(self, model): self.model = model\n",
        "\n",
        "  def summary(self):\n",
        "    in_shape = self.getInShape()\n",
        "    model = self.getModel()\n",
        "    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3])\n",
        "    for layer in model:\n",
        "      print(\"\\tin_shape:\", type(x), x.shape)\n",
        "      print(type(layer), sys.getsizeof(layer))\n",
        "      x = layer(x)\n",
        "    print(\"out_shape:\", type(x), x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7_sw1k3k4pr"
      },
      "source": [
        "##### YoloBackbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.519558Z",
          "iopub.status.busy": "2024-03-10T03:46:58.519284Z",
          "iopub.status.idle": "2024-03-10T03:46:58.528646Z",
          "shell.execute_reply": "2024-03-10T03:46:58.527761Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.519535Z"
        },
        "trusted": true,
        "id": "0NT7ITSLk4pr"
      },
      "outputs": [],
      "source": [
        "class YoloBackbone(nnModule):\n",
        "  \"\"\"YOLO backbone extract feature from the input\"\"\"\n",
        "\n",
        "  def __init__(self, in_shpae: tuple, backbone_config=YOLO_BACKBONE_ARCHITECTURE):\n",
        "    super(YoloBackbone, self).__init__()\n",
        "    self.setInShape(in_shpae)\n",
        "    model = nn.ModuleList()\n",
        "    x = torch.rand(in_shpae[0], in_shpae[1], in_shpae[2], in_shpae[3])\n",
        "    for i, config in enumerate(backbone_config):\n",
        "      if type(config) == tuple:\n",
        "        out_c, k_size, stride, _ = config\n",
        "        model += [ConvWithBatchNorm(in_c=x.shape[1], out_c=out_c, k_size=k_size, stride=stride, negative_slope=0.1)]\n",
        "        x = model[-1](x)\n",
        "\n",
        "      elif type(config) == str:\n",
        "        model += [nn.MaxPool2d(kernel_size=2, stride=2, padding=0)]\n",
        "        x = model[-1](x)\n",
        "\n",
        "      elif type(config) == list:\n",
        "        out_ces, num_repeat = config\n",
        "        model += [BottleNeckBlock(x.shape[1], out_ces, num_repeat)]\n",
        "        x = model[-1](x)\n",
        "    self.setOutShape(x.shape)\n",
        "    self.setModel(model=model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.getModel():\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7uJuu6Ak4ps",
        "outputId": "4c79895d-5035-4f0e-81db-32e6aad61c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 3, 224, 224])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 64, 112, 112])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 64, 56, 56])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 192, 56, 56])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 192, 28, 28])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 128, 28, 28])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 256, 28, 28])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 512, 28, 28])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 512, 14, 14])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 512, 14, 14])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 1024, 14, 14])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 1024, 7, 7])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "out_shape: <class 'torch.Tensor'> torch.Size([1, 1024, 7, 7])\n"
          ]
        }
      ],
      "source": [
        "model = YoloBackbone((1, 3, 224, 224)).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyoC7afXk4ps"
      },
      "source": [
        "##### YoloOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qXT-6EExk4ps"
      },
      "outputs": [],
      "source": [
        "YOLO_OUT_ARCHITECTURE = [(4096, 0.1), 0.5, (2040, 0.1), 0.5, (1024, 0.1), 0.5, (GRID_SIZE * GRID_SIZE * (NUM_BOXES * 5 + NUM_CLASSES), 0.1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.943891Z",
          "iopub.status.busy": "2024-03-10T03:46:58.943518Z",
          "iopub.status.idle": "2024-03-10T03:46:58.954778Z",
          "shell.execute_reply": "2024-03-10T03:46:58.953883Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.943858Z"
        },
        "trusted": true,
        "id": "rqf_8jvrk4pt"
      },
      "outputs": [],
      "source": [
        "class YoloOutput(nnModule):\n",
        "  \"\"\"YOLO last convolution and FC layers to produce prediction\"\"\"\n",
        "\n",
        "  def __init__(self, in_shape: tuple):\n",
        "    super(YoloOutput, self).__init__()\n",
        "    self.setInShape(in_shape=in_shape)\n",
        "    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3])\n",
        "    model = nn.ModuleList()\n",
        "    model += [ConvWithBatchNorm(in_shape[1], out_c=1024, k_size=3),\n",
        "              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n",
        "              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n",
        "              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n",
        "              nn.Flatten()]\n",
        "    for layer in model: x = layer(x)\n",
        "\n",
        "    for i, config in enumerate(YOLO_OUT_ARCHITECTURE):\n",
        "      if type(config) == tuple:\n",
        "        out_f, slop = config\n",
        "        model += [nn.Linear(in_features=x.shape[1], out_features=out_f), nn.LeakyReLU(negative_slope=slop)]\n",
        "        x = model[-1](model[-2](x))\n",
        "\n",
        "      else:\n",
        "        p = config\n",
        "        model += [nn.Dropout(p=0.5)]\n",
        "        x = model[-1](x)\n",
        "    self.setOutShape(x.shape)\n",
        "    self.setModel(model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.getModel():\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtBJgOFhk4pt",
        "outputId": "86677286-a58c-46c8-b80e-bf2bb6819808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class 'torch.nn.modules.flatten.Flatten'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 50176])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 4096])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 4096])\n",
            "<class 'torch.nn.modules.dropout.Dropout'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 4096])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 2040])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 2040])\n",
            "<class 'torch.nn.modules.dropout.Dropout'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 2040])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024])\n",
            "<class 'torch.nn.modules.dropout.Dropout'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 637])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "out_shape: <class 'torch.Tensor'> torch.Size([16, 637])\n"
          ]
        }
      ],
      "source": [
        "YoloOutput((16, 1024, 7, 7)).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cthP7XbNk4pt"
      },
      "source": [
        "##### YoloV1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:59.980516Z",
          "iopub.status.busy": "2024-03-10T03:46:59.979894Z",
          "iopub.status.idle": "2024-03-10T03:46:59.989798Z",
          "shell.execute_reply": "2024-03-10T03:46:59.988801Z",
          "shell.execute_reply.started": "2024-03-10T03:46:59.980480Z"
        },
        "trusted": true,
        "id": "ynLF5B34k4pt"
      },
      "outputs": [],
      "source": [
        "class YoloV1(nnModule):\n",
        "  \"\"\"End-to-end YOLO network\"\"\"\n",
        "\n",
        "  def __init__(self, in_shape: tuple):\n",
        "    super(YoloV1, self).__init__()\n",
        "    self.setInShape(in_shape)\n",
        "\n",
        "    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3])\n",
        "    yolo_backbone = YoloBackbone(in_shape)\n",
        "    x = yolo_backbone(x)\n",
        "    yolo_output = YoloOutput(in_shape=x.shape)\n",
        "    x = yolo_output(x)\n",
        "\n",
        "    self.setOutShape(x.shape)\n",
        "    model = nn.ModuleList()\n",
        "    model += [yolo_backbone, yolo_output]\n",
        "    self.setModel(model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.getModel():\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiyS607xk4pu",
        "outputId": "b782536e-214e-48f5-e81c-8a1413bc8e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 3, 224, 224])\n",
            "<class '__main__.YoloBackbone'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.YoloOutput'> 48\n",
            "out_shape: <class 'torch.Tensor'> torch.Size([16, 637])\n"
          ]
        }
      ],
      "source": [
        "YoloV1((16, 3, 224, 224)).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4pfk6uRk4pu"
      },
      "source": [
        "### YoloLoss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n",
        "  \"\"\"\n",
        "  Calculates intersection over union\n",
        "\n",
        "  Parameters:\n",
        "      boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "      boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "      box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.\n",
        "\n",
        "  Returns:\n",
        "      tensor: Intersection over union for all examples\n",
        "  \"\"\"\n",
        "  # boxes_preds shape is (N, 4)\n",
        "  # boxes_labels shape is (N, 4)\n",
        "\n",
        "  if box_format == 'midpoint':\n",
        "      box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "      box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "      box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "      box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "\n",
        "      box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "      box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "      box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "      box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "  if box_format == 'corners':\n",
        "      box1_x1 = boxes_preds[..., 0:1]\n",
        "      box1_y1 = boxes_preds[..., 1:2]\n",
        "      box1_x2 = boxes_preds[..., 2:3]\n",
        "      box1_y2 = boxes_preds[..., 3:4]\n",
        "\n",
        "      box2_x1 = boxes_labels[..., 0:1]\n",
        "      box2_y1 = boxes_labels[..., 1:2]\n",
        "      box2_x2 = boxes_labels[..., 2:3]\n",
        "      box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "  x1 = torch.max(box1_x1, box2_x1)\n",
        "  y1 = torch.max(box1_y1, box2_y1)\n",
        "  x2 = torch.min(box1_x2, box2_x2)\n",
        "  y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "  #$$$.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
        "  intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "  box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "  box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "  return intersection / (box1_area + box2_area - intersection + 1e-6)\n"
      ],
      "metadata": {
        "id": "aZpFmnwzNzRw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPS = 1e-6\n",
        "def sign_sqrt(pred):\n",
        "  return torch.sign(pred) * torch.sqrt(torch.abs(pred + EPS))\n",
        "### !!! tai 0 khong co dao ham cua abs"
      ],
      "metadata": {
        "id": "vDYzFCrMpqu7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:59.991771Z",
          "iopub.status.busy": "2024-03-10T03:46:59.991405Z",
          "iopub.status.idle": "2024-03-10T03:47:00.014653Z",
          "shell.execute_reply": "2024-03-10T03:47:00.013718Z",
          "shell.execute_reply.started": "2024-03-10T03:46:59.991737Z"
        },
        "trusted": true,
        "id": "q8AApm83k4pu"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "  def __init__(self, coord_c=5, noobj_c=0.5):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.COORD = coord_c\n",
        "    self.NOOBJ = noobj_c\n",
        "    self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "  def forward(self, predictions: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    predictions = predictions.reshape((-1, GRID_SIZE, GRID_SIZE, NUM_BOXES * 5 + NUM_CLASSES))\n",
        "    exists_box = target[..., [4]]\n",
        "    iou_b1 = intersection_over_union(\n",
        "        predictions[...,0:4], target[..., 0:4])\n",
        "    iou_b2 = intersection_over_union(\n",
        "        predictions[..., 5:9], target[..., 0:4])\n",
        "    bestbox = torch.where(iou_b1 >= iou_b2, 0, 1)\n",
        "\n",
        "    # class loss\n",
        "    class_loss = self.mse(\n",
        "      exists_box * predictions[..., 10:],\n",
        "      exists_box * target[..., 5:])\n",
        "    print(class_loss)\n",
        "\n",
        "    # obj loss\n",
        "    pred_box = (\n",
        "        (1-bestbox) * predictions[..., [4]] + (bestbox) * predictions[..., [9]]\n",
        "    )\n",
        "    object_loss = self.mse(\n",
        "      exists_box * pred_box,\n",
        "      exists_box * target[..., [4]]\n",
        "    )\n",
        "    print(object_loss)\n",
        "\n",
        "    # coor loss\n",
        "    pred_box = (\n",
        "        (1-bestbox) * predictions[..., 0:4] + (bestbox) * predictions[..., 5:9]\n",
        "    )\n",
        "    true_box = target[..., 0:4]\n",
        "    pred_box[..., 2:4] = sign_sqrt(pred_box[..., 2:4])\n",
        "    true_box[..., 2:4] = sign_sqrt(true_box[..., 2:4])\n",
        "    coor_loss = self.mse(\n",
        "      #exists_box * pred_box, end_dim=-2),\n",
        "      #exists_box * true_box, end_dim=-2),\n",
        "      exists_box * pred_box, exists_box * true_box\n",
        "    )\n",
        "    print(coor_loss)\n",
        "\n",
        "    # no obj loss\n",
        "    no_obj_loss = self.mse(\n",
        "      (1 - exists_box) * predictions[..., [4]], (1 - exists_box) * target[..., [4]]\n",
        "    )\n",
        "    no_obj_loss += self.mse(\n",
        "      (1 - exists_box) * predictions[..., [9]], (1 - exists_box) * target[..., [4]]\n",
        "    )\n",
        "    print(no_obj_loss)\n",
        "    return class_loss + object_loss + self.COORD * coor_loss + self.NOOBJ * no_obj_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:00.026680Z",
          "iopub.status.busy": "2024-03-10T03:47:00.026374Z",
          "iopub.status.idle": "2024-03-10T03:47:00.840781Z",
          "shell.execute_reply": "2024-03-10T03:47:00.839556Z",
          "shell.execute_reply.started": "2024-03-10T03:47:00.026609Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9J8TssPk4pv",
        "outputId": "2181a76a-6bd8-49dd-a80b-bb9825629183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__: imgs= (<class 'torch.Tensor'>, torch.Size([16, 3, 224, 224]), '80Bytes')\n",
            "__verbose__: pred= (<class 'torch.Tensor'>, torch.Size([16, 637]), '80Bytes')\n"
          ]
        }
      ],
      "source": [
        "imgs = torch.rand(16, 3, 224, 224)\n",
        "over(imgs, \"imgs=\")\n",
        "model = YoloV1((16, 3, 224, 224))\n",
        "out = model(imgs)\n",
        "over(out, \"pred=\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:00.842523Z",
          "iopub.status.busy": "2024-03-10T03:47:00.842145Z",
          "iopub.status.idle": "2024-03-10T03:47:01.075822Z",
          "shell.execute_reply": "2024-03-10T03:47:01.074914Z",
          "shell.execute_reply.started": "2024-03-10T03:47:00.842489Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLR5FUOQk4pv",
        "outputId": "42d9816a-32b0-4ed1-8e8c-8680d40c0a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__: true= (<class 'torch.Tensor'>, torch.Size([16, 7, 7, 8]), '80Bytes')\n",
            "tensor(256.4862, grad_fn=<MseLossBackward0>)\n",
            "tensor(154.1761, grad_fn=<MseLossBackward0>)\n",
            "tensor(398.1599, grad_fn=<MseLossBackward0>)\n",
            "tensor(44.9569, grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2423.9402, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "out_true = torch.rand(16, 7, 7, 8)\n",
        "over(out_true, \"true=\")\n",
        "loss = YoloLoss()\n",
        "loss.forward(out, out_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUZaJA6Kk4pw"
      },
      "source": [
        "### DataLoad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:01.077602Z",
          "iopub.status.busy": "2024-03-10T03:47:01.077296Z",
          "iopub.status.idle": "2024-03-10T03:47:01.112647Z",
          "shell.execute_reply": "2024-03-10T03:47:01.111724Z",
          "shell.execute_reply.started": "2024-03-10T03:47:01.077577Z"
        },
        "trusted": true,
        "id": "Ffxoi8Bgk4pw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from xml.etree import ElementTree\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "class_names = ['apple', 'banana', 'orange']\n",
        "\n",
        "class DataLoad(data.Dataset):\n",
        "  def __init__(self, file_dir, repeat, aug=False) -> None:\n",
        "    super().__init__()\n",
        "    dataframe = self.get_dataframe(file_dir=file_dir)\n",
        "    self.imgs, self.labels = self.load_dataset(dataframe, input_shape=(224, 224, 3), #!!!\n",
        "                                                grid_size=GRID_SIZE) # np.ndarray\n",
        "    # repeat\n",
        "    for i in range(repeat):\n",
        "      self.imgs = np.concatenate((self.imgs, self.imgs), axis=0)\n",
        "      self.labels = np.concatenate((self.labels, self.labels), axis=0)\n",
        "    # aug\n",
        "    if(aug == True):\n",
        "      for i, img in enumerate(self.imgs):\n",
        "        label = self.labels[i]\n",
        "        self.imgs[i], self.labels[i] = self._apply_augmentation(img, label, seed=RANDOM_STATE)\n",
        "    over(self.imgs, \"DataLoad > __init__ > imgs=\")\n",
        "    over(self.labels, \"DataLoad > __init__ > labels=\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      x, y = self.imgs[idx], self.labels[idx] # np.ndarray\n",
        "      x, y = tf.convert_to_tensor(x), tf.convert_to_tensor(y) # tf.tensor\n",
        "      # if(self.aug == True):\n",
        "      #   x, y = self._apply_augmentation(x, y, seed=RANDOM_STATE) # tf.tensor\n",
        "      # cast type\n",
        "      x = torch.tensor(x.numpy(), dtype=torch.float32)  # torch.tensor\n",
        "      y = torch.tensor(y.numpy(), dtype=torch.float32)\n",
        "      return x, y\n",
        "\n",
        "\n",
        "  def get_dataframe(self, file_dir):\n",
        "    \"\"\"\n",
        "    Get the train/val/test dataframe which contains image\n",
        "    file names and annotations files. If `phase = train',\n",
        "    return train and val set\n",
        "    :param file_dir: File directory to create dataframe\n",
        "    :return file_df: Train or test dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    img_files = [os.path.join(file_dir, img_file) for img_file\n",
        "                 in sorted(os.listdir(file_dir)) if img_file[-4:] == '.jpg']\n",
        "    annot_files = [img_file[:-4] + '.xml' for img_file in img_files]\n",
        "\n",
        "    img_file_series = pd.Series(img_files, name='Image_file')\n",
        "    annot_file_series = pd.Series(annot_files, name='Annotation_file')\n",
        "    file_df = pd.DataFrame(pd.concat([img_file_series, annot_file_series], axis=1))\n",
        "    return file_df\n",
        "\n",
        "  def prepare_image(self, filename, input_shape):\n",
        "    \"\"\"\n",
        "    Resize image to expected dimension, and opt. apply some random transformation.\n",
        "    :param filename: File name\n",
        "    :param input_shape: Shape expected by the model (image will be resize accordingly)\n",
        "    :return : 3D image array, pixel values from [0., 1.]\n",
        "    \"\"\"\n",
        "\n",
        "    img = img_to_array(load_img(filename, target_size=input_shape)) / 255.\n",
        "    img = np.einsum('ijk->kij', img)\n",
        "    # print(img.shape)\n",
        "    return img\n",
        "\n",
        "  def convert_to_xywh(self, bboxes):\n",
        "    \"\"\"\n",
        "    Convert list of (xmin, ymin, xmax, ymax) to\n",
        "    (x_center, y_center, box_width, box_height)\n",
        "    :param bboxes: List of bounding boxes, each has 4\n",
        "    values (xmin, ymin, xmax, ymax)\n",
        "    :return boxes: List of bounding boxes, each has 4\n",
        "    values (x_center, y_center, box_width, box_height)\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = list()\n",
        "    for box in bboxes:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "\n",
        "        # Compute width and height of box\n",
        "        box_width = xmax - xmin\n",
        "        box_height = ymax - ymin\n",
        "\n",
        "        # Compute x, y center\n",
        "        x_center = int(xmin + (box_width / 2))\n",
        "        y_center = int(ymin + (box_height / 2))\n",
        "\n",
        "        boxes.append((x_center, y_center, box_width, box_height))\n",
        "\n",
        "    return boxes\n",
        "\n",
        "  def extract_annotation_file(self, filename):\n",
        "    \"\"\"\n",
        "    Extract bounding boxes from an annotation file\n",
        "    :param filename: Annotation file name\n",
        "    :return boxes: List of bounding boxes in image, each box has\n",
        "    4 values (x_center, y_center, box_width, box_height)\n",
        "    :return classes: List of classes in image\n",
        "    :return width: Width of image\n",
        "    :return height: Height of image\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and parse the file\n",
        "    tree = ElementTree.parse(filename)\n",
        "    # Get the root of the document\n",
        "    root = tree.getroot()\n",
        "    boxes = list()\n",
        "    classes = list()\n",
        "\n",
        "    # Extract each bounding box\n",
        "    for box in root.findall('.//object'):\n",
        "        cls = class_names.index(box.find('name').text)\n",
        "        xmin = int(box.find('bndbox/xmin').text)\n",
        "        ymin = int(box.find('bndbox/ymin').text)\n",
        "        xmax = int(box.find('bndbox/xmax').text)\n",
        "        ymax = int(box.find('bndbox/ymax').text)\n",
        "        coors = (xmin, ymin, xmax, ymax)\n",
        "        boxes.append(coors)\n",
        "        classes.append(cls)\n",
        "\n",
        "    boxes = self.convert_to_xywh(boxes)\n",
        "\n",
        "    # Get width and height of an image\n",
        "    width = int(root.find('.//size/width').text)\n",
        "    height = int(root.find('.//size/height').text)\n",
        "\n",
        "    # Some annotation files have set width and height by 0,\n",
        "    # so we need to load image and get it width and height\n",
        "    if (width == 0) or (height == 0):\n",
        "        img = load_img(filename[:-4] + '.jpg')\n",
        "        width, height = img.width, img.height\n",
        "\n",
        "    return boxes, classes, width, height\n",
        "\n",
        "  def convert_bboxes_to_tensor(self, bboxes, classes, img_width, img_height, grid_size=7):\n",
        "    \"\"\"\n",
        "    Convert list of bounding boxes to tensor target\n",
        "    :param bboxes: List of bounding boxes in image, each box has\n",
        "    4 values (x_center, y_center, box_width, box_height)\n",
        "    :param classes: List of class in image\n",
        "    :param img_width: Image's width\n",
        "    :param img_height: Image's height\n",
        "    :param grid_size: Grid size\n",
        "    :return target: Target tensor (grid_size x grid_size x (5 + num_classes))\n",
        "    \"\"\"\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "    target = np.zeros(shape=(grid_size, grid_size, 5 + num_classes), dtype=np.float32)\n",
        "\n",
        "    for idx, bbox in enumerate(bboxes):\n",
        "        x_center, y_center, width, height = bbox\n",
        "\n",
        "        # Compute size of each cell in grid\n",
        "        cell_w, cell_h = img_width / grid_size, img_height / grid_size\n",
        "\n",
        "        # Determine cell i, j of bounding box\n",
        "        i, j = int(y_center / cell_h), int(x_center / cell_w)\n",
        "\n",
        "        # Compute value of x_center and y_center in cell\n",
        "        x, y = (x_center / cell_w) - j, (y_center / cell_h) - i\n",
        "\n",
        "        # Normalize width and height of bounding box\n",
        "        w_norm, h_norm = width / img_width, height / img_height\n",
        "\n",
        "        # Add bounding box to tensor\n",
        "        # Set x, y, w, h\n",
        "        target[i, j, :4] += (x, y, w_norm, h_norm)\n",
        "        # Set obj score\n",
        "        target[i, j, 4] = 1.\n",
        "        # Set class dist.\n",
        "        target[i, j, 5 + classes[idx]] = 1.\n",
        "    return target\n",
        "\n",
        "  def load_dataset(self, dataframe, input_shape, grid_size=7):\n",
        "    \"\"\"\n",
        "    Load img and target tensor\n",
        "    :param dataframe: Dataframe contains img files and annotation files\n",
        "    :param input_shape: Shape expected by the model (image will be resize accordingly)\n",
        "    :param grid_size: Grid size\n",
        "    :return dataset: Iterable dataset\n",
        "    \"\"\"\n",
        "\n",
        "    imgs, targets = list(), list()\n",
        "\n",
        "    for _, row in tqdm(dataframe.iterrows()):\n",
        "        img = self.prepare_image(row.Image_file, input_shape)\n",
        "        target = self.extract_annotation_file(row.Annotation_file)\n",
        "        target = self.convert_bboxes_to_tensor(*target, grid_size)\n",
        "        imgs.append(img)\n",
        "        targets.append(target)\n",
        "\n",
        "    imgs = np.array(imgs)\n",
        "    targets = np.array(targets)\n",
        "    return imgs, targets\n",
        "    # dataset = tf.data.Dataset.from_tensor_slices((imgs, targets))\n",
        "    # return dataset\n",
        "\n",
        "  def _apply_augmentation(self, image, target, seed=None):\n",
        "    \"\"\"\n",
        "    Apply random brightness and saturation on image\n",
        "    :param image: Image to augment\n",
        "    :param target: Target tensor\n",
        "    :param seed: Seed for random operation\n",
        "    :return : Processed data\n",
        "    \"\"\"\n",
        "\n",
        "    # Random bright & saturation change\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1, seed=seed)\n",
        "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5, seed=seed)\n",
        "\n",
        "    # Keeping pixel values in check\n",
        "    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "    return image, target\n",
        "\n",
        "  def load_dataset_from_df(self, dataframe, batch_size=32, num_repeat=None, shuffle=False,\n",
        "                         input_shape=(448, 448, 3), grid_size=7, augment=False,\n",
        "                         seed=None):\n",
        "    \"\"\"\n",
        "    Instantiate dataset\n",
        "    :param dataframe: Dataframe contains img files and annotation files\n",
        "    :param batch_size: Batch size\n",
        "    :param num_epochs: Number of epochs (to repeat the iteration - infinite if None)\n",
        "    :param shuffle: Flag to shuffle the dataset (if True)\n",
        "    :param input_shape: Shape of the processed image\n",
        "    :param grid_size: Grid size\n",
        "    :param augment: Flag to apply some random augmentations to the image\n",
        "    :param seed: Random seed for operation\n",
        "    :return : Iterable dataset\n",
        "    \"\"\"\n",
        "\n",
        "    apply_augmentation = partial(self._apply_augmentation, seed=seed)\n",
        "    dataset = self.load_dataset(dataframe, input_shape, grid_size)\n",
        "    ### !!!\n",
        "    dataset = dataset.repeat(num_repeat)\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1000, seed)\n",
        "    if augment:\n",
        "        dataset = dataset.map(apply_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:01.114229Z",
          "iopub.status.busy": "2024-03-10T03:47:01.113825Z",
          "iopub.status.idle": "2024-03-10T03:47:01.127777Z",
          "shell.execute_reply": "2024-03-10T03:47:01.126753Z",
          "shell.execute_reply.started": "2024-03-10T03:47:01.114201Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGPrJnsLk4py",
        "outputId": "f8ec3baa-c7d3-45a3-ac62-3d2d803525cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3, 224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "IN_SHAPE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oNYux31ctz8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492a7f62-fb07-49b8-e4a8-feb5ecdb7341"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:01.129581Z",
          "iopub.status.busy": "2024-03-10T03:47:01.129181Z",
          "iopub.status.idle": "2024-03-10T03:47:04.085670Z",
          "shell.execute_reply": "2024-03-10T03:47:04.084832Z",
          "shell.execute_reply.started": "2024-03-10T03:47:01.129547Z"
        },
        "trusted": true,
        "id": "g7aCKNwWk4py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0ab6eb-edc9-4b99-f10f-f2a202367eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "21it [00:01, 14.75it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "240it [00:05, 40.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__: DataLoad > __init__ > imgs= (<class 'numpy.ndarray'>, (3840, 3, 224, 224), '2312110240Bytes')\n",
            "__verbose__: DataLoad > __init__ > labels= (<class 'numpy.ndarray'>, (3840, 7, 7, 8), '6021280Bytes')\n"
          ]
        }
      ],
      "source": [
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/My_Laptop_Data/fruits_dataset/train'\n",
        "dataload = DataLoad(train_dir, aug=False, repeat=4)\n",
        "train_df = dataload.get_dataframe(train_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:04.087088Z",
          "iopub.status.busy": "2024-03-10T03:47:04.086785Z",
          "iopub.status.idle": "2024-03-10T03:47:04.092216Z",
          "shell.execute_reply": "2024-03-10T03:47:04.091248Z",
          "shell.execute_reply.started": "2024-03-10T03:47:04.087063Z"
        },
        "trusted": true,
        "id": "Y6bnDILPk4pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41d8f26-eb66-4cc0-e98c-57825c1d0b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__:  (<class '__main__.DataLoad'>, 'no_shape', '48Bytes')\n",
            "__verbose__:  (<class 'torch.Tensor'>, torch.Size([3, 224, 224]), '80Bytes')\n",
            "__verbose__:  (<class 'torch.Tensor'>, torch.Size([16, 7, 7, 8]), '80Bytes')\n"
          ]
        }
      ],
      "source": [
        "over(dataload)\n",
        "over(dataload[0][0])\n",
        "over(dataload[0:16][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:05.169394Z",
          "iopub.status.busy": "2024-03-10T03:47:05.169010Z",
          "iopub.status.idle": "2024-03-10T03:47:05.173886Z",
          "shell.execute_reply": "2024-03-10T03:47:05.172894Z",
          "shell.execute_reply.started": "2024-03-10T03:47:05.169351Z"
        },
        "trusted": true,
        "id": "jNUQ__YOk4p0"
      },
      "outputs": [],
      "source": [
        "# Assuming train_dataset is your training dataset\n",
        "# train_loader = DataLoader(dataset=dataload, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True, prefetch_factor=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:05.175464Z",
          "iopub.status.busy": "2024-03-10T03:47:05.175168Z",
          "iopub.status.idle": "2024-03-10T03:47:05.180973Z",
          "shell.execute_reply": "2024-03-10T03:47:05.180101Z",
          "shell.execute_reply.started": "2024-03-10T03:47:05.175440Z"
        },
        "trusted": true,
        "id": "QX3qRl7Pk4p0"
      },
      "outputs": [],
      "source": [
        "# Assuming train_dataset is your training dataset\n",
        "train_loader = data.DataLoader(dataset=dataload, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "over(train_loader, \"train_loader=\")"
      ],
      "metadata": {
        "id": "Ds3HlEGU1OnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53be5fa-990b-4ff5-f747-b993b8e05dd0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__: train_loader= (<class 'torch.utils.data.dataloader.DataLoader'>, 'no_shape', '48Bytes')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9fYLiHwk4qC"
      },
      "source": [
        "### training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:48:45.967884Z",
          "iopub.status.busy": "2024-03-10T03:48:45.967230Z",
          "iopub.status.idle": "2024-03-10T03:48:45.981803Z",
          "shell.execute_reply": "2024-03-10T03:48:45.980885Z",
          "shell.execute_reply.started": "2024-03-10T03:48:45.967829Z"
        },
        "trusted": true,
        "id": "oUiDcu5jk4qF"
      },
      "outputs": [],
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  mean_loss = []\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    out = model(x).to(DEVICE)\n",
        "    over(out, \"train_fn > out=\")\n",
        "    over(y, \"train_fn > y=\")\n",
        "    loss = loss_fn(out, y)\n",
        "    print(\"loss in batch\", loss.detach().numpy())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:49:32.743596Z",
          "iopub.status.busy": "2024-03-10T03:49:32.742756Z",
          "iopub.status.idle": "2024-03-10T03:49:32.770236Z",
          "shell.execute_reply": "2024-03-10T03:49:32.769009Z",
          "shell.execute_reply.started": "2024-03-10T03:49:32.743565Z"
        },
        "trusted": true,
        "id": "zZTm0-S-k4qG"
      },
      "outputs": [],
      "source": [
        "train_loader = data.DataLoader(dataset=dataload, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "model = YoloV1(in_shape=IN_SHAPE).to(DEVICE)\n",
        "optimizer = optim.Adam(list(model.parameters()), lr=2e-5, weight_decay=0)\n",
        "loss_fn = YoloLoss().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_fn(train_loader, model, optimizer, loss_fn)"
      ],
      "metadata": {
        "id": "euzVKy3--qkM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1446a9da-de6e-41ab-a49c-db57469de55e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/960 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(10.4106, grad_fn=<MseLossBackward0>)\n",
            "tensor(10.3511, grad_fn=<MseLossBackward0>)\n",
            "tensor(16.4696, grad_fn=<MseLossBackward0>)\n",
            "tensor(1.1594, grad_fn=<AddBackward0>)\n",
            "loss in batch 103.68942\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(8.6974, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.8443, grad_fn=<MseLossBackward0>)\n",
            "tensor(14.0548, grad_fn=<MseLossBackward0>)\n",
            "tensor(1.5393, grad_fn=<AddBackward0>)\n",
            "loss in batch 88.58521\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(4.9224, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.4851, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.4686, grad_fn=<MseLossBackward0>)\n",
            "tensor(1.7359, grad_fn=<AddBackward0>)\n",
            "loss in batch 52.618557\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(7.3975, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.6330, grad_fn=<MseLossBackward0>)\n",
            "tensor(10.6924, grad_fn=<MseLossBackward0>)\n",
            "tensor(2.9149, grad_fn=<AddBackward0>)\n",
            "loss in batch 69.95002\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(5.9302, grad_fn=<MseLossBackward0>)\n",
            "tensor(5.6657, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.9873, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.1075, grad_fn=<AddBackward0>)\n",
            "loss in batch 53.586105\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(5.4107, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.7995, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.3076, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.7266, grad_fn=<AddBackward0>)\n",
            "loss in batch 49.111614\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(8.5284, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.3860, grad_fn=<MseLossBackward0>)\n",
            "tensor(9.3482, grad_fn=<MseLossBackward0>)\n",
            "tensor(5.7573, grad_fn=<AddBackward0>)\n",
            "loss in batch 66.533966\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(7.1899, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.4237, grad_fn=<MseLossBackward0>)\n",
            "tensor(5.8967, grad_fn=<MseLossBackward0>)\n",
            "tensor(5.8206, grad_fn=<AddBackward0>)\n",
            "loss in batch 46.007458\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(6.6058, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.5925, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.1059, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.2548, grad_fn=<AddBackward0>)\n",
            "loss in batch 52.355373\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(8.4811, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.0091, grad_fn=<MseLossBackward0>)\n",
            "tensor(11.6790, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.4410, grad_fn=<AddBackward0>)\n",
            "loss in batch 79.105865\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(5.1492, grad_fn=<MseLossBackward0>)\n",
            "tensor(3.3428, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.9629, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.5663, grad_fn=<AddBackward0>)\n",
            "loss in batch 52.089787\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(6.1360, grad_fn=<MseLossBackward0>)\n",
            "tensor(5.9841, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.4439, grad_fn=<MseLossBackward0>)\n",
            "tensor(9.5754, grad_fn=<AddBackward0>)\n",
            "loss in batch 49.12761\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(9.5988, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.8479, grad_fn=<MseLossBackward0>)\n",
            "tensor(10.3970, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.9024, grad_fn=<AddBackward0>)\n",
            "loss in batch 73.88266\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(7.3496, grad_fn=<MseLossBackward0>)\n",
            "tensor(5.6722, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.6874, grad_fn=<MseLossBackward0>)\n",
            "tensor(12.5918, grad_fn=<AddBackward0>)\n",
            "loss in batch 42.754833\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(4.4769, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.0008, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.4841, grad_fn=<MseLossBackward0>)\n",
            "tensor(11.0212, grad_fn=<AddBackward0>)\n",
            "loss in batch 36.408936\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(7.2391, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.3175, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.3320, grad_fn=<MseLossBackward0>)\n",
            "tensor(12.5759, grad_fn=<AddBackward0>)\n",
            "loss in batch 56.5045\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(2.5283, grad_fn=<MseLossBackward0>)\n",
            "tensor(3.6451, grad_fn=<MseLossBackward0>)\n",
            "tensor(2.4211, grad_fn=<MseLossBackward0>)\n",
            "tensor(12.1312, grad_fn=<AddBackward0>)\n",
            "loss in batch 24.344448\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(13.1627, grad_fn=<MseLossBackward0>)\n",
            "tensor(12.0480, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.8232, grad_fn=<MseLossBackward0>)\n",
            "tensor(14.5101, grad_fn=<AddBackward0>)\n",
            "loss in batch 76.5821\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(6.7336, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.1335, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.1705, grad_fn=<MseLossBackward0>)\n",
            "tensor(14.3602, grad_fn=<AddBackward0>)\n",
            "loss in batch 55.899467\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(6.2737, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.6188, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.0460, grad_fn=<MseLossBackward0>)\n",
            "tensor(11.9921, grad_fn=<AddBackward0>)\n",
            "loss in batch 49.118282\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(7.7266, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.9153, grad_fn=<MseLossBackward0>)\n",
            "tensor(7.0671, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.8405, grad_fn=<AddBackward0>)\n",
            "loss in batch 54.397602\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(9.5143, grad_fn=<MseLossBackward0>)\n",
            "tensor(6.6387, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.3089, grad_fn=<MseLossBackward0>)\n",
            "tensor(14.1335, grad_fn=<AddBackward0>)\n",
            "loss in batch 64.76406\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(3.7917, grad_fn=<MseLossBackward0>)\n",
            "tensor(3.2034, grad_fn=<MseLossBackward0>)\n",
            "tensor(3.5621, grad_fn=<MseLossBackward0>)\n",
            "tensor(12.7380, grad_fn=<AddBackward0>)\n",
            "loss in batch 31.17482\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(5.7988, grad_fn=<MseLossBackward0>)\n",
            "tensor(3.7694, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.7691, grad_fn=<MseLossBackward0>)\n",
            "tensor(12.6017, grad_fn=<AddBackward0>)\n",
            "loss in batch 39.714405\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(4.2236, grad_fn=<MseLossBackward0>)\n",
            "tensor(4.0326, grad_fn=<MseLossBackward0>)\n",
            "tensor(3.2058, grad_fn=<MseLossBackward0>)\n",
            "tensor(12.5463, grad_fn=<AddBackward0>)\n",
            "loss in batch 30.558401\n",
            "__verbose__: train_fn > out= (<class 'torch.Tensor'>, torch.Size([4, 637]), '80Bytes')\n",
            "__verbose__: train_fn > y= (<class 'torch.Tensor'>, torch.Size([4, 7, 7, 8]), '80Bytes')\n",
            "tensor(7.8848, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.0160, grad_fn=<MseLossBackward0>)\n",
            "tensor(8.0959, grad_fn=<MseLossBackward0>)\n",
            "tensor(13.3316, grad_fn=<AddBackward0>)\n",
            "loss in batch 63.04622\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-855632263aa8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-0e855a2b9a75>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean loss was {sum(mean_loss) / len(mean_loss)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 state_steps)\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    312\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(1, 7, 7, 2)\n",
        "print(a)\n",
        "sign_sqrt(a)"
      ],
      "metadata": {
        "id": "dS6q2OteQBeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHcY1XR9k4qG"
      },
      "source": [
        "### End"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4569729,
          "sourceId": 7803789,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}