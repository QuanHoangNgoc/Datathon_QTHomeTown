{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:51.435671Z",
          "iopub.status.busy": "2024-03-10T03:46:51.434958Z",
          "iopub.status.idle": "2024-03-10T03:46:51.447220Z",
          "shell.execute_reply": "2024-03-10T03:46:51.446250Z",
          "shell.execute_reply.started": "2024-03-10T03:46:51.435639Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vymEjuKIk4pd",
        "outputId": "c189ecb7-be37-4bfc-a935-a64187e575da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nice\n"
          ]
        }
      ],
      "source": [
        "print(\"nice\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMCb_MwNk4pf"
      },
      "source": [
        "### global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:51.449179Z",
          "iopub.status.busy": "2024-03-10T03:46:51.448900Z",
          "iopub.status.idle": "2024-03-10T03:46:58.091874Z",
          "shell.execute_reply": "2024-03-10T03:46:58.090861Z",
          "shell.execute_reply.started": "2024-03-10T03:46:51.449158Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjC16i_mk4pi",
        "outputId": "1519d964-7e13-409e-8cf2-89770e012315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "print(tf.__version__)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.095890Z",
          "iopub.status.busy": "2024-03-10T03:46:58.095283Z",
          "iopub.status.idle": "2024-03-10T03:46:58.105202Z",
          "shell.execute_reply": "2024-03-10T03:46:58.104215Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.095854Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf2_vC6Kk4pj",
        "outputId": "b27b17ad-a1b2-4ea1-dd1d-d7c3cfa4f8c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.25.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7eddb6d36f50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n",
        "# setting random_state\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db9CUa5Ek4pl"
      },
      "source": [
        "### some libraries and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.107279Z",
          "iopub.status.busy": "2024-03-10T03:46:58.106432Z",
          "iopub.status.idle": "2024-03-10T03:46:58.452054Z",
          "shell.execute_reply": "2024-03-10T03:46:58.451227Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.107253Z"
        },
        "trusted": true,
        "id": "r7nwh0e_k4pm"
      },
      "outputs": [],
      "source": [
        "# libraries\n",
        "import sys, math\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.454862Z",
          "iopub.status.busy": "2024-03-10T03:46:58.454295Z",
          "iopub.status.idle": "2024-03-10T03:46:58.462716Z",
          "shell.execute_reply": "2024-03-10T03:46:58.461718Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.454817Z"
        },
        "trusted": true,
        "id": "om06RJL2k4pm"
      },
      "outputs": [],
      "source": [
        "# fix random_state\n",
        "def fixRandomState(fixed_state: int=RANDOM_STATE):\n",
        "  np.random.seed(fixed_state)\n",
        "  tf.random.set_seed(fixed_state)\n",
        "  torch.manual_seed(fixed_state)\n",
        "\n",
        "# exception\n",
        "def exception(requirement: bool, content):\n",
        "  if(requirement == False): raise ValueError(content)\n",
        "def catchException(ex: Exception):\n",
        "  print(type(ex), ex.args)\n",
        "  exception(False, ex)\n",
        "\n",
        "# message\n",
        "def mesVerbose(flag: bool, verbose, func_dir: str=\"\"):\n",
        "  if(flag == False): return\n",
        "  print(\"__verbose__:\", func_dir, verbose)\n",
        "def mesWarning(note, func_dir: str=\"\"):\n",
        "  print(\"__warning__:\", func_dir, str(note) + \"###\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.464382Z",
          "iopub.status.busy": "2024-03-10T03:46:58.464053Z",
          "iopub.status.idle": "2024-03-10T03:46:58.472781Z",
          "shell.execute_reply": "2024-03-10T03:46:58.471931Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.464349Z"
        },
        "trusted": true,
        "id": "1lElqLSLk4pn"
      },
      "outputs": [],
      "source": [
        "def over(val, name=\"\") -> tuple:\n",
        "  try: mesVerbose(True, (type(val), val.shape, str(sys.getsizeof(val)) + \"Bytes\"), name)\n",
        "  except: mesVerbose(True, (type(val), \"no_shape\", str(sys.getsizeof(val)) + \"Bytes\"), name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSU76Uxgk4po"
      },
      "source": [
        "### model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.485961Z",
          "iopub.status.busy": "2024-03-10T03:46:58.485664Z",
          "iopub.status.idle": "2024-03-10T03:46:58.493427Z",
          "shell.execute_reply": "2024-03-10T03:46:58.492461Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.485938Z"
        },
        "trusted": true,
        "id": "LXmFP8FWk4pp"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "from torch.utils import data\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "IN_SHAPE = (BATCH_SIZE, 3, 224, 224)\n",
        "\n",
        "YOLO_BACKBONE_ARCHITECTURE = [(64, 7, 2, 'same'), 'M',\n",
        "                                (192, 3, 1, 'same'), 'M',\n",
        "                                (128, 1, 1, 'valid'),\n",
        "                                [(128, 256), 1],\n",
        "                                [(256, 512), 1], 'M',\n",
        "                                [(256, 512), 4],\n",
        "                                [(512, 1024), 1], 'M',\n",
        "                                [(512, 1024), 2]]\n",
        "\n",
        "GRID_SIZE = 7\n",
        "NUM_BOXES = 2\n",
        "NUM_CLASSES = 3\n",
        "OUT_SHAPE = (16, 7, 7, 13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:00.016141Z",
          "iopub.status.busy": "2024-03-10T03:47:00.015793Z",
          "iopub.status.idle": "2024-03-10T03:47:00.025351Z",
          "shell.execute_reply": "2024-03-10T03:47:00.024521Z",
          "shell.execute_reply.started": "2024-03-10T03:47:00.016112Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SZN5eJek4pv",
        "outputId": "6bcae5ce-ee74-4953-aeae-1bc2ddd58271"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5MIjFYCk4pp"
      },
      "source": [
        "##### blcoks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.495263Z",
          "iopub.status.busy": "2024-03-10T03:46:58.494890Z",
          "iopub.status.idle": "2024-03-10T03:46:58.506457Z",
          "shell.execute_reply": "2024-03-10T03:46:58.505472Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.495210Z"
        },
        "trusted": true,
        "id": "r5mESj1bk4pq"
      },
      "outputs": [],
      "source": [
        "class ConvWithBatchNorm(nn.Module):\n",
        "  \"\"\"Conv layer with batch norm and leaky relu\"\"\"\n",
        "\n",
        "  def __init__(self, in_c: int, out_c: int, k_size: int, stride=1, negative_slope=0.1):\n",
        "    super(ConvWithBatchNorm, self).__init__()\n",
        "    self.in_shape = ()\n",
        "    self.out_shape = ()\n",
        "\n",
        "    padding = k_size // 2\n",
        "    layers = nn.ModuleList()\n",
        "    layers += [nn.Conv2d(in_c, out_c, k_size, stride=stride, padding=padding, bias=False)]\n",
        "    layers += [nn.BatchNorm2d(num_features=out_c)]\n",
        "    layers += [nn.LeakyReLU(negative_slope=negative_slope)]\n",
        "    self.layers = layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.in_shape = x.shape\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out_shape = x.shape\n",
        "    return x\n",
        "\n",
        "  def getInShape(self): return self.in_shape\n",
        "  def getOutShape(self): return self.out_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.507903Z",
          "iopub.status.busy": "2024-03-10T03:46:58.507592Z",
          "iopub.status.idle": "2024-03-10T03:46:58.515625Z",
          "shell.execute_reply": "2024-03-10T03:46:58.514681Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.507874Z"
        },
        "trusted": true,
        "id": "Nj0k_pkUk4pq"
      },
      "outputs": [],
      "source": [
        "class BottleNeckBlock(nn.Module):\n",
        "  \"\"\"Block of 1x1 reduction layers followed by 3x3 conv. layer\"\"\"\n",
        "\n",
        "  def __init__(self, in_c: int, out_ces: tuple, num_repeat: int):\n",
        "    super(BottleNeckBlock, self).__init__()\n",
        "    self. out_shape = ()\n",
        "\n",
        "    out_1x1 = out_ces[0]\n",
        "    out_3x3 = out_ces[1]\n",
        "    layers = nn.ModuleList()\n",
        "    for i in range(num_repeat):\n",
        "      layers += [nn.Conv2d(in_c, out_1x1, 1, stride=1, padding=0, bias=False)]\n",
        "      layers += [nn.Conv2d(out_1x1, out_3x3, 3, stride=1, padding=1, bias=False)]\n",
        "    self.layers = layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out_shape = x.shape\n",
        "    return x\n",
        "\n",
        "  def getOutShape(self): return self.out_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QXDFK-vck4pr"
      },
      "outputs": [],
      "source": [
        "class nnModule(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super(nnModule, self).__init__()\n",
        "    self.in_shape = ()\n",
        "    self.out_shape = ()\n",
        "    self.model = nn.ModuleList()\n",
        "\n",
        "  def getInShape(self): return self.in_shape\n",
        "  def getOutShape(self): return self.out_shape\n",
        "  def getModel(self): return self.model\n",
        "  def setInShape(self, in_shape): self.in_shape = in_shape\n",
        "  def setOutShape(self, out_shape): self.out_shape = out_shape\n",
        "  def setModel(self, model): self.model = model\n",
        "\n",
        "  def summary(self):\n",
        "    in_shape = self.getInShape()\n",
        "    model = self.getModel()\n",
        "    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3])\n",
        "    for layer in model:\n",
        "      print(\"\\tin_shape:\", type(x), x.shape)\n",
        "      print(type(layer), sys.getsizeof(layer))\n",
        "      x = layer(x)\n",
        "    print(\"out_shape:\", type(x), x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7_sw1k3k4pr"
      },
      "source": [
        "##### YoloBackbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.519558Z",
          "iopub.status.busy": "2024-03-10T03:46:58.519284Z",
          "iopub.status.idle": "2024-03-10T03:46:58.528646Z",
          "shell.execute_reply": "2024-03-10T03:46:58.527761Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.519535Z"
        },
        "trusted": true,
        "id": "0NT7ITSLk4pr"
      },
      "outputs": [],
      "source": [
        "class YoloBackbone(nnModule):\n",
        "  \"\"\"YOLO backbone extract feature from the input\"\"\"\n",
        "\n",
        "  def __init__(self, in_shpae: tuple, backbone_config=YOLO_BACKBONE_ARCHITECTURE):\n",
        "    super(YoloBackbone, self).__init__()\n",
        "    self.setInShape(in_shpae)\n",
        "    model = nn.ModuleList()\n",
        "    x = torch.rand(in_shpae[0], in_shpae[1], in_shpae[2], in_shpae[3])\n",
        "    for i, config in enumerate(backbone_config):\n",
        "      if type(config) == tuple:\n",
        "        out_c, k_size, stride, _ = config\n",
        "        model += [ConvWithBatchNorm(in_c=x.shape[1], out_c=out_c, k_size=k_size, stride=stride, negative_slope=0.1)]\n",
        "        x = model[-1](x)\n",
        "\n",
        "      elif type(config) == str:\n",
        "        model += [nn.MaxPool2d(kernel_size=2, stride=2, padding=0)]\n",
        "        x = model[-1](x)\n",
        "\n",
        "      elif type(config) == list:\n",
        "        out_ces, num_repeat = config\n",
        "        model += [BottleNeckBlock(x.shape[1], out_ces, num_repeat)]\n",
        "        x = model[-1](x)\n",
        "    self.setOutShape(x.shape)\n",
        "    self.setModel(model=model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.getModel():\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7uJuu6Ak4ps",
        "outputId": "694dcf94-9e69-42bd-d42e-112519e2a976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 3, 224, 224])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 64, 112, 112])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 64, 56, 56])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 192, 56, 56])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 192, 28, 28])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 128, 28, 28])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 256, 28, 28])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 512, 28, 28])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 512, 14, 14])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 512, 14, 14])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 1024, 14, 14])\n",
            "<class 'torch.nn.modules.pooling.MaxPool2d'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([1, 1024, 7, 7])\n",
            "<class '__main__.BottleNeckBlock'> 48\n",
            "out_shape: <class 'torch.Tensor'> torch.Size([1, 1024, 7, 7])\n"
          ]
        }
      ],
      "source": [
        "model = YoloBackbone((1, 3, 224, 224)).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyoC7afXk4ps"
      },
      "source": [
        "##### YoloOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qXT-6EExk4ps"
      },
      "outputs": [],
      "source": [
        "YOLO_OUT_ARCHITECTURE = [(4096, 0.1), 0.5, (2040, 0.1), 0.5, (1024, 0.1), 0.5, (GRID_SIZE * GRID_SIZE * (NUM_BOXES * 5 + NUM_CLASSES), 0.1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:58.943891Z",
          "iopub.status.busy": "2024-03-10T03:46:58.943518Z",
          "iopub.status.idle": "2024-03-10T03:46:58.954778Z",
          "shell.execute_reply": "2024-03-10T03:46:58.953883Z",
          "shell.execute_reply.started": "2024-03-10T03:46:58.943858Z"
        },
        "trusted": true,
        "id": "rqf_8jvrk4pt"
      },
      "outputs": [],
      "source": [
        "class YoloOutput(nnModule):\n",
        "  \"\"\"YOLO last convolution and FC layers to produce prediction\"\"\"\n",
        "\n",
        "  def __init__(self, in_shape: tuple):\n",
        "    super(YoloOutput, self).__init__()\n",
        "    self.setInShape(in_shape=in_shape)\n",
        "    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3])\n",
        "    model = nn.ModuleList()\n",
        "    model += [ConvWithBatchNorm(in_shape[1], out_c=1024, k_size=3),\n",
        "              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n",
        "              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n",
        "              ConvWithBatchNorm(1024, out_c=1024, k_size=3),\n",
        "              nn.Flatten()]\n",
        "    for layer in model: x = layer(x)\n",
        "\n",
        "    for i, config in enumerate(YOLO_OUT_ARCHITECTURE):\n",
        "      if type(config) == tuple:\n",
        "        out_f, slop = config\n",
        "        model += [nn.Linear(in_features=x.shape[1], out_features=out_f), nn.LeakyReLU(negative_slope=slop)]\n",
        "        x = model[-1](model[-2](x))\n",
        "\n",
        "      else:\n",
        "        p = config\n",
        "        model += [nn.Dropout(p=0.5)]\n",
        "        x = model[-1](x)\n",
        "    self.setOutShape(x.shape)\n",
        "    self.setModel(model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.getModel():\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtBJgOFhk4pt",
        "outputId": "20b496af-28f7-46ea-e127-7a7028cf5948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.ConvWithBatchNorm'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class 'torch.nn.modules.flatten.Flatten'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 50176])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 4096])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 4096])\n",
            "<class 'torch.nn.modules.dropout.Dropout'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 4096])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 2040])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 2040])\n",
            "<class 'torch.nn.modules.dropout.Dropout'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 2040])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024])\n",
            "<class 'torch.nn.modules.dropout.Dropout'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024])\n",
            "<class 'torch.nn.modules.linear.Linear'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 637])\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 48\n",
            "out_shape: <class 'torch.Tensor'> torch.Size([16, 637])\n"
          ]
        }
      ],
      "source": [
        "YoloOutput((16, 1024, 7, 7)).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cthP7XbNk4pt"
      },
      "source": [
        "##### YoloV1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:59.980516Z",
          "iopub.status.busy": "2024-03-10T03:46:59.979894Z",
          "iopub.status.idle": "2024-03-10T03:46:59.989798Z",
          "shell.execute_reply": "2024-03-10T03:46:59.988801Z",
          "shell.execute_reply.started": "2024-03-10T03:46:59.980480Z"
        },
        "trusted": true,
        "id": "ynLF5B34k4pt"
      },
      "outputs": [],
      "source": [
        "class YoloV1(nnModule):\n",
        "  \"\"\"End-to-end YOLO network\"\"\"\n",
        "\n",
        "  def __init__(self, in_shape: tuple):\n",
        "    super(YoloV1, self).__init__()\n",
        "    self.setInShape(in_shape)\n",
        "\n",
        "    x = torch.rand(in_shape[0], in_shape[1], in_shape[2], in_shape[3])\n",
        "    yolo_backbone = YoloBackbone(in_shape)\n",
        "    x = yolo_backbone(x)\n",
        "    yolo_output = YoloOutput(in_shape=x.shape)\n",
        "    x = yolo_output(x)\n",
        "\n",
        "    self.setOutShape(x.shape)\n",
        "    model = nn.ModuleList()\n",
        "    model += [yolo_backbone, yolo_output]\n",
        "    self.setModel(model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.getModel():\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiyS607xk4pu",
        "outputId": "3131eeec-0d3c-4de4-9d34-d803ea5fb278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 3, 224, 224])\n",
            "<class '__main__.YoloBackbone'> 48\n",
            "\tin_shape: <class 'torch.Tensor'> torch.Size([16, 1024, 7, 7])\n",
            "<class '__main__.YoloOutput'> 48\n",
            "out_shape: <class 'torch.Tensor'> torch.Size([16, 637])\n"
          ]
        }
      ],
      "source": [
        "YoloV1((16, 3, 224, 224)).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4pfk6uRk4pu"
      },
      "source": [
        "### YoloLoss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n",
        "  \"\"\"\n",
        "  Calculates intersection over union\n",
        "\n",
        "  Parameters:\n",
        "      boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "      boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "      box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.\n",
        "\n",
        "  Returns:\n",
        "      tensor: Intersection over union for all examples\n",
        "  \"\"\"\n",
        "  # boxes_preds shape is (N, 4)\n",
        "  # boxes_labels shape is (N, 4)\n",
        "\n",
        "  if box_format == 'midpoint':\n",
        "      box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "      box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "      box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "      box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "\n",
        "      box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "      box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "      box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "      box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "  if box_format == 'corners':\n",
        "      box1_x1 = boxes_preds[..., 0:1]\n",
        "      box1_y1 = boxes_preds[..., 1:2]\n",
        "      box1_x2 = boxes_preds[..., 2:3]\n",
        "      box1_y2 = boxes_preds[..., 3:4]\n",
        "\n",
        "      box2_x1 = boxes_labels[..., 0:1]\n",
        "      box2_y1 = boxes_labels[..., 1:2]\n",
        "      box2_x2 = boxes_labels[..., 2:3]\n",
        "      box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "  x1 = torch.max(box1_x1, box2_x1)\n",
        "  y1 = torch.max(box1_y1, box2_y1)\n",
        "  x2 = torch.min(box1_x2, box2_x2)\n",
        "  y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "  #$$$.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
        "  intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "  box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "  box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "  return intersection / (box1_area + box2_area - intersection + 1e-6)\n"
      ],
      "metadata": {
        "id": "aZpFmnwzNzRw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:46:59.991771Z",
          "iopub.status.busy": "2024-03-10T03:46:59.991405Z",
          "iopub.status.idle": "2024-03-10T03:47:00.014653Z",
          "shell.execute_reply": "2024-03-10T03:47:00.013718Z",
          "shell.execute_reply.started": "2024-03-10T03:46:59.991737Z"
        },
        "trusted": true,
        "id": "q8AApm83k4pu"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "  def __init__(self, coord_c=5, noobj_c=0.5):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.COORD = coord_c\n",
        "    self.NOOBJ = noobj_c\n",
        "    self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "  def forward(self, predictions: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    predictions = predictions.reshape((-1, GRID_SIZE, GRID_SIZE, NUM_BOXES * 5 + NUM_CLASSES))\n",
        "    exists_box = target[..., [4]]\n",
        "    iou_b1 = intersection_over_union(\n",
        "        predictions[...,0:4], target[..., 0:4])\n",
        "    iou_b2 = intersection_over_union(\n",
        "        predictions[..., 5:9], target[..., 0:4])\n",
        "    bestbox = torch.where(iou_b1 >= iou_b2, 0, 1)\n",
        "\n",
        "    # class loss\n",
        "    class_loss = self.mse(\n",
        "      exists_box * predictions[..., 10:],\n",
        "      exists_box * target[..., 5:])\n",
        "    print(class_loss)\n",
        "\n",
        "    # obj loss\n",
        "    pred_box = (\n",
        "            bestbox * predictions[..., [4]] + (1 - bestbox) * predictions[..., [9]]\n",
        "        )\n",
        "    object_loss = self.mse(\n",
        "        torch.flatten(exists_box * pred_box),\n",
        "        torch.flatten(exists_box * target[..., [4]]),\n",
        "    )\n",
        "    print(object_loss)\n",
        "\n",
        "    return class_loss + object_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:00.026680Z",
          "iopub.status.busy": "2024-03-10T03:47:00.026374Z",
          "iopub.status.idle": "2024-03-10T03:47:00.840781Z",
          "shell.execute_reply": "2024-03-10T03:47:00.839556Z",
          "shell.execute_reply.started": "2024-03-10T03:47:00.026609Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9J8TssPk4pv",
        "outputId": "5de27eab-8dd6-4356-bfa2-51445c9030f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__: imgs= (<class 'torch.Tensor'>, torch.Size([16, 3, 224, 224]), '80Bytes')\n",
            "__verbose__: pred= (<class 'torch.Tensor'>, torch.Size([16, 637]), '80Bytes')\n"
          ]
        }
      ],
      "source": [
        "imgs = torch.rand(16, 3, 224, 224)\n",
        "over(imgs, \"imgs=\")\n",
        "model = YoloV1((16, 3, 224, 224))\n",
        "out = model(imgs)\n",
        "over(out, \"pred=\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:00.842523Z",
          "iopub.status.busy": "2024-03-10T03:47:00.842145Z",
          "iopub.status.idle": "2024-03-10T03:47:01.075822Z",
          "shell.execute_reply": "2024-03-10T03:47:01.074914Z",
          "shell.execute_reply.started": "2024-03-10T03:47:00.842489Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLR5FUOQk4pv",
        "outputId": "c5dd6e0c-b11a-43fc-b496-261634157fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__verbose__: true= (<class 'torch.Tensor'>, torch.Size([16, 7, 7, 8]), '80Bytes')\n",
            "tensor(245.3689, grad_fn=<MseLossBackward0>)\n",
            "tensor(139.9247, grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(385.2935, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "out_true = torch.rand(16, 7, 7, 8)\n",
        "over(out_true, \"true=\")\n",
        "loss = YoloLoss()\n",
        "loss.forward(out, out_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUZaJA6Kk4pw"
      },
      "source": [
        "### DataLoad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:01.077602Z",
          "iopub.status.busy": "2024-03-10T03:47:01.077296Z",
          "iopub.status.idle": "2024-03-10T03:47:01.112647Z",
          "shell.execute_reply": "2024-03-10T03:47:01.111724Z",
          "shell.execute_reply.started": "2024-03-10T03:47:01.077577Z"
        },
        "trusted": true,
        "id": "Ffxoi8Bgk4pw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from xml.etree import ElementTree\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "class_names = ['apple', 'banana', 'orange']\n",
        "\n",
        "class DataLoad(data.Dataset):\n",
        "  def __init__(self, file_dir, repeat, aug=False) -> None:\n",
        "    super().__init__()\n",
        "    dataframe = self.get_dataframe(file_dir=file_dir)\n",
        "    self.imgs, self.labels = self.load_dataset(dataframe, input_shape=(224, 224, 3), #!!!\n",
        "                                                grid_size=GRID_SIZE) # np.ndarray\n",
        "    # repeat\n",
        "    for i in range(repeat):\n",
        "      self.imgs = np.concatenate((self.imgs, self.imgs), axis=0)\n",
        "      self.labels = np.concatenate((self.labels, self.labels), axis=0)\n",
        "    # aug\n",
        "    if(aug == True):\n",
        "      for i, img in enumerate(self.imgs):\n",
        "        label = self.labels[i]\n",
        "        self.imgs[i], self.labels[i] = self._apply_augmentation(img, label, seed=RANDOM_STATE)\n",
        "    over(self.imgs)\n",
        "    over(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      x, y = self.imgs[idx], self.labels[idx] # np.ndarray\n",
        "      x, y = tf.convert_to_tensor(x), tf.convert_to_tensor(y) # tf.tensor\n",
        "      # if(self.aug == True):\n",
        "      #   x, y = self._apply_augmentation(x, y, seed=RANDOM_STATE) # tf.tensor\n",
        "      # cast type\n",
        "      x = torch.tensor(x.numpy(), dtype=torch.float32)  # torch.tensor\n",
        "      y = torch.tensor(y.numpy(), dtype=torch.float32)\n",
        "      return x, y\n",
        "\n",
        "\n",
        "  def get_dataframe(self, file_dir):\n",
        "    \"\"\"\n",
        "    Get the train/val/test dataframe which contains image\n",
        "    file names and annotations files. If `phase = train',\n",
        "    return train and val set\n",
        "    :param file_dir: File directory to create dataframe\n",
        "    :return file_df: Train or test dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    img_files = [os.path.join(file_dir, img_file) for img_file\n",
        "                 in sorted(os.listdir(file_dir)) if img_file[-4:] == '.jpg']\n",
        "    annot_files = [img_file[:-4] + '.xml' for img_file in img_files]\n",
        "\n",
        "    img_file_series = pd.Series(img_files, name='Image_file')\n",
        "    annot_file_series = pd.Series(annot_files, name='Annotation_file')\n",
        "    file_df = pd.DataFrame(pd.concat([img_file_series, annot_file_series], axis=1))\n",
        "    return file_df\n",
        "\n",
        "  def prepare_image(self, filename, input_shape):\n",
        "    \"\"\"\n",
        "    Resize image to expected dimension, and opt. apply some random transformation.\n",
        "    :param filename: File name\n",
        "    :param input_shape: Shape expected by the model (image will be resize accordingly)\n",
        "    :return : 3D image array, pixel values from [0., 1.]\n",
        "    \"\"\"\n",
        "\n",
        "    img = img_to_array(load_img(filename, target_size=input_shape)) / 255.\n",
        "    img = np.einsum('ijk->kij', img)\n",
        "    # print(img.shape)\n",
        "    return img\n",
        "\n",
        "  def convert_to_xywh(self, bboxes):\n",
        "    \"\"\"\n",
        "    Convert list of (xmin, ymin, xmax, ymax) to\n",
        "    (x_center, y_center, box_width, box_height)\n",
        "    :param bboxes: List of bounding boxes, each has 4\n",
        "    values (xmin, ymin, xmax, ymax)\n",
        "    :return boxes: List of bounding boxes, each has 4\n",
        "    values (x_center, y_center, box_width, box_height)\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = list()\n",
        "    for box in bboxes:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "\n",
        "        # Compute width and height of box\n",
        "        box_width = xmax - xmin\n",
        "        box_height = ymax - ymin\n",
        "\n",
        "        # Compute x, y center\n",
        "        x_center = int(xmin + (box_width / 2))\n",
        "        y_center = int(ymin + (box_height / 2))\n",
        "\n",
        "        boxes.append((x_center, y_center, box_width, box_height))\n",
        "\n",
        "    return boxes\n",
        "\n",
        "  def extract_annotation_file(self, filename):\n",
        "    \"\"\"\n",
        "    Extract bounding boxes from an annotation file\n",
        "    :param filename: Annotation file name\n",
        "    :return boxes: List of bounding boxes in image, each box has\n",
        "    4 values (x_center, y_center, box_width, box_height)\n",
        "    :return classes: List of classes in image\n",
        "    :return width: Width of image\n",
        "    :return height: Height of image\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and parse the file\n",
        "    tree = ElementTree.parse(filename)\n",
        "    # Get the root of the document\n",
        "    root = tree.getroot()\n",
        "    boxes = list()\n",
        "    classes = list()\n",
        "\n",
        "    # Extract each bounding box\n",
        "    for box in root.findall('.//object'):\n",
        "        cls = class_names.index(box.find('name').text)\n",
        "        xmin = int(box.find('bndbox/xmin').text)\n",
        "        ymin = int(box.find('bndbox/ymin').text)\n",
        "        xmax = int(box.find('bndbox/xmax').text)\n",
        "        ymax = int(box.find('bndbox/ymax').text)\n",
        "        coors = (xmin, ymin, xmax, ymax)\n",
        "        boxes.append(coors)\n",
        "        classes.append(cls)\n",
        "\n",
        "    boxes = self.convert_to_xywh(boxes)\n",
        "\n",
        "    # Get width and height of an image\n",
        "    width = int(root.find('.//size/width').text)\n",
        "    height = int(root.find('.//size/height').text)\n",
        "\n",
        "    # Some annotation files have set width and height by 0,\n",
        "    # so we need to load image and get it width and height\n",
        "    if (width == 0) or (height == 0):\n",
        "        img = load_img(filename[:-4] + '.jpg')\n",
        "        width, height = img.width, img.height\n",
        "\n",
        "    return boxes, classes, width, height\n",
        "\n",
        "  def convert_bboxes_to_tensor(self, bboxes, classes, img_width, img_height, grid_size=7):\n",
        "    \"\"\"\n",
        "    Convert list of bounding boxes to tensor target\n",
        "    :param bboxes: List of bounding boxes in image, each box has\n",
        "    4 values (x_center, y_center, box_width, box_height)\n",
        "    :param classes: List of class in image\n",
        "    :param img_width: Image's width\n",
        "    :param img_height: Image's height\n",
        "    :param grid_size: Grid size\n",
        "    :return target: Target tensor (grid_size x grid_size x (5 + num_classes))\n",
        "    \"\"\"\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "    target = np.zeros(shape=(grid_size, grid_size, 5 + num_classes), dtype=np.float32)\n",
        "\n",
        "    for idx, bbox in enumerate(bboxes):\n",
        "        x_center, y_center, width, height = bbox\n",
        "\n",
        "        # Compute size of each cell in grid\n",
        "        cell_w, cell_h = img_width / grid_size, img_height / grid_size\n",
        "\n",
        "        # Determine cell i, j of bounding box\n",
        "        i, j = int(y_center / cell_h), int(x_center / cell_w)\n",
        "\n",
        "        # Compute value of x_center and y_center in cell\n",
        "        x, y = (x_center / cell_w) - j, (y_center / cell_h) - i\n",
        "\n",
        "        # Normalize width and height of bounding box\n",
        "        w_norm, h_norm = width / img_width, height / img_height\n",
        "\n",
        "        # Add bounding box to tensor\n",
        "        # Set x, y, w, h\n",
        "        target[i, j, :4] += (x, y, w_norm, h_norm)\n",
        "        # Set obj score\n",
        "        target[i, j, 4] = 1.\n",
        "        # Set class dist.\n",
        "        target[i, j, 5 + classes[idx]] = 1.\n",
        "    return target\n",
        "\n",
        "  def load_dataset(self, dataframe, input_shape, grid_size=7):\n",
        "    \"\"\"\n",
        "    Load img and target tensor\n",
        "    :param dataframe: Dataframe contains img files and annotation files\n",
        "    :param input_shape: Shape expected by the model (image will be resize accordingly)\n",
        "    :param grid_size: Grid size\n",
        "    :return dataset: Iterable dataset\n",
        "    \"\"\"\n",
        "\n",
        "    imgs, targets = list(), list()\n",
        "\n",
        "    for _, row in tqdm(dataframe.iterrows()):\n",
        "        img = self.prepare_image(row.Image_file, input_shape)\n",
        "        target = self.extract_annotation_file(row.Annotation_file)\n",
        "        target = self.convert_bboxes_to_tensor(*target, grid_size)\n",
        "        imgs.append(img)\n",
        "        targets.append(target)\n",
        "\n",
        "    imgs = np.array(imgs)\n",
        "    targets = np.array(targets)\n",
        "    return imgs, targets\n",
        "    # dataset = tf.data.Dataset.from_tensor_slices((imgs, targets))\n",
        "    # return dataset\n",
        "\n",
        "  def _apply_augmentation(self, image, target, seed=None):\n",
        "    \"\"\"\n",
        "    Apply random brightness and saturation on image\n",
        "    :param image: Image to augment\n",
        "    :param target: Target tensor\n",
        "    :param seed: Seed for random operation\n",
        "    :return : Processed data\n",
        "    \"\"\"\n",
        "\n",
        "    # Random bright & saturation change\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1, seed=seed)\n",
        "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5, seed=seed)\n",
        "\n",
        "    # Keeping pixel values in check\n",
        "    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "    return image, target\n",
        "\n",
        "  def load_dataset_from_df(self, dataframe, batch_size=32, num_repeat=None, shuffle=False,\n",
        "                         input_shape=(448, 448, 3), grid_size=7, augment=False,\n",
        "                         seed=None):\n",
        "    \"\"\"\n",
        "    Instantiate dataset\n",
        "    :param dataframe: Dataframe contains img files and annotation files\n",
        "    :param batch_size: Batch size\n",
        "    :param num_epochs: Number of epochs (to repeat the iteration - infinite if None)\n",
        "    :param shuffle: Flag to shuffle the dataset (if True)\n",
        "    :param input_shape: Shape of the processed image\n",
        "    :param grid_size: Grid size\n",
        "    :param augment: Flag to apply some random augmentations to the image\n",
        "    :param seed: Random seed for operation\n",
        "    :return : Iterable dataset\n",
        "    \"\"\"\n",
        "\n",
        "    apply_augmentation = partial(self._apply_augmentation, seed=seed)\n",
        "    dataset = self.load_dataset(dataframe, input_shape, grid_size)\n",
        "    ### !!!\n",
        "    dataset = dataset.repeat(num_repeat)\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1000, seed)\n",
        "    if augment:\n",
        "        dataset = dataset.map(apply_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:01.114229Z",
          "iopub.status.busy": "2024-03-10T03:47:01.113825Z",
          "iopub.status.idle": "2024-03-10T03:47:01.127777Z",
          "shell.execute_reply": "2024-03-10T03:47:01.126753Z",
          "shell.execute_reply.started": "2024-03-10T03:47:01.114201Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGPrJnsLk4py",
        "outputId": "fac07190-5413-4a12-8bc6-ce9d0477c43b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3, 224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "IN_SHAPE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oNYux31ctz8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:01.129581Z",
          "iopub.status.busy": "2024-03-10T03:47:01.129181Z",
          "iopub.status.idle": "2024-03-10T03:47:04.085670Z",
          "shell.execute_reply": "2024-03-10T03:47:04.084832Z",
          "shell.execute_reply.started": "2024-03-10T03:47:01.129547Z"
        },
        "trusted": true,
        "id": "g7aCKNwWk4py"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/My_Laptop_Data/fruits_dataset/train'\n",
        "dataload = DataLoad(train_dir, aug=False, repeat=4)\n",
        "train_df = dataload.get_dataframe(train_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:04.087088Z",
          "iopub.status.busy": "2024-03-10T03:47:04.086785Z",
          "iopub.status.idle": "2024-03-10T03:47:04.092216Z",
          "shell.execute_reply": "2024-03-10T03:47:04.091248Z",
          "shell.execute_reply.started": "2024-03-10T03:47:04.087063Z"
        },
        "trusted": true,
        "id": "Y6bnDILPk4pz"
      },
      "outputs": [],
      "source": [
        "over(dataload)\n",
        "over(dataload[0][0])\n",
        "over(dataload[0:16][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:05.169394Z",
          "iopub.status.busy": "2024-03-10T03:47:05.169010Z",
          "iopub.status.idle": "2024-03-10T03:47:05.173886Z",
          "shell.execute_reply": "2024-03-10T03:47:05.172894Z",
          "shell.execute_reply.started": "2024-03-10T03:47:05.169351Z"
        },
        "trusted": true,
        "id": "jNUQ__YOk4p0"
      },
      "outputs": [],
      "source": [
        "# Assuming train_dataset is your training dataset\n",
        "# train_loader = DataLoader(dataset=dataload, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True, prefetch_factor=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:47:05.175464Z",
          "iopub.status.busy": "2024-03-10T03:47:05.175168Z",
          "iopub.status.idle": "2024-03-10T03:47:05.180973Z",
          "shell.execute_reply": "2024-03-10T03:47:05.180101Z",
          "shell.execute_reply.started": "2024-03-10T03:47:05.175440Z"
        },
        "trusted": true,
        "id": "QX3qRl7Pk4p0"
      },
      "outputs": [],
      "source": [
        "# Assuming train_dataset is your training dataset\n",
        "train_loader = data.DataLoader(dataset=dataload, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "over(train_loader, \"train_loader=\")"
      ],
      "metadata": {
        "id": "Ds3HlEGU1OnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9fYLiHwk4qC"
      },
      "source": [
        "### training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:48:45.967884Z",
          "iopub.status.busy": "2024-03-10T03:48:45.967230Z",
          "iopub.status.idle": "2024-03-10T03:48:45.981803Z",
          "shell.execute_reply": "2024-03-10T03:48:45.980885Z",
          "shell.execute_reply.started": "2024-03-10T03:48:45.967829Z"
        },
        "trusted": true,
        "id": "oUiDcu5jk4qF"
      },
      "outputs": [],
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  mean_loss = []\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    out = model(x).to(DEVICE)\n",
        "    over(out, \"train_fn > out=\")\n",
        "    over(y, \"train_fn > y=\")\n",
        "    loss = loss_fn(out, y)\n",
        "    print(\"class_loss\", loss.detach().numpy())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T03:49:32.743596Z",
          "iopub.status.busy": "2024-03-10T03:49:32.742756Z",
          "iopub.status.idle": "2024-03-10T03:49:32.770236Z",
          "shell.execute_reply": "2024-03-10T03:49:32.769009Z",
          "shell.execute_reply.started": "2024-03-10T03:49:32.743565Z"
        },
        "trusted": true,
        "id": "zZTm0-S-k4qG"
      },
      "outputs": [],
      "source": [
        "train_loader = data.DataLoader(dataset=dataload, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "model = YoloV1(in_shape=IN_SHAPE).to(DEVICE)\n",
        "optimizer = optim.Adam(list(model.parameters()), lr=2e-5, weight_decay=0)\n",
        "loss_fn = YoloLoss().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_fn(train_loader, model, optimizer, loss_fn)"
      ],
      "metadata": {
        "id": "euzVKy3--qkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHcY1XR9k4qG"
      },
      "source": [
        "### End"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4569729,
          "sourceId": 7803789,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}